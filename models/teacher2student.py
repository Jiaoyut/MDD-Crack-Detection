import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
import logging
import time
import math
import hashlib
import numpy as np
from collections import defaultdict, deque
import cv2

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'


class CorrectEbbinghausDistillation:
    """æ­£ç¡®çš„è‰¾å®¾æµ©æ–¯è’¸é¦ç³»ç»Ÿ - ä½¿ç”¨æœ‰æ•ˆçš„YOLOå‚æ•°"""

    def __init__(self, teacher_path, student_config, data_config, device='cuda'):
        self.device = device
        self.teacher_path = teacher_path
        self.student_config = student_config
        self.data_config = data_config

        # è®­ç»ƒçŠ¶æ€
        self.step_count = 0
        self.epoch_count = 0
        self.distill_applied = 0
        self.start_time = time.time()

        # ç¨³å®šåŒ–å‚æ•°
        self.nan_detected = False
        self.nan_recovery_steps = 0
        self.stable_mode = False

        # æ­£ç¡®çš„è®°å¿†ç³»ç»Ÿ
        self.memory_model = CorrectMemoryModel()
        self.review_scheduler = CorrectReviewScheduler(self.memory_model)

        self.setup_correct_models()
        self.setup_correct_callbacks()

        logging.info("ğŸ§ âœ… æ­£ç¡®çš„è‰¾å®¾æµ©æ–¯è’¸é¦ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def setup_correct_models(self):
        """è®¾ç½®æ­£ç¡®æ¨¡å‹"""
        try:
            # æ•™å¸ˆæ¨¡å‹
            self.teacher = YOLO(self.teacher_path)
            self.teacher.model.to(self.device).eval()
            for p in self.teacher.model.parameters():
                p.requires_grad = False

            # å­¦ç”Ÿæ¨¡å‹
            self.student = YOLO(self.student_config)
            self.student.model.to(self.device)

            # æ·»åŠ æ¢¯åº¦è£å‰ª
            self._add_correct_gradient_clipping()

            logging.info("âœ… æ­£ç¡®æ¨¡å‹è®¾ç½®å®Œæˆ")
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            raise

    def _add_correct_gradient_clipping(self):
        """æ·»åŠ æ­£ç¡®çš„æ¢¯åº¦è£å‰ª"""
        try:
            # æ‰‹åŠ¨æ·»åŠ æ¢¯åº¦è£å‰ªé’©å­
            for name, param in self.student.model.named_parameters():
                if param.requires_grad:
                    def make_hook(param_name):
                        def hook(grad):
                            clipped_grad = torch.clamp(grad, -1.0, 1.0)
                            if self.step_count % 500 == 0:
                                grad_norm = grad.norm().item() if grad is not None else 0
                                clipped_norm = clipped_grad.norm().item() if clipped_grad is not None else 0
                                if grad_norm > 1.0:
                                    logging.info(
                                        f"âœ‚ï¸ æ¢¯åº¦è£å‰ª: {param_name} æ¢¯åº¦èŒƒæ•° {grad_norm:.3f} -> {clipped_norm:.3f}")
                            return clipped_grad

                        return hook

                    param.register_hook(make_hook(name))

            logging.info("âœ… æ¢¯åº¦è£å‰ªé’©å­å·²æ·»åŠ ")
        except Exception as e:
            logging.warning(f"âŒ æ¢¯åº¦è£å‰ªè®¾ç½®å¤±è´¥: {e}")

    def setup_correct_callbacks(self):
        """è®¾ç½®æ­£ç¡®å›è°ƒ"""
        logging.info("ğŸ”§ è®¾ç½®æ­£ç¡®å›è°ƒ...")

        # ä¿å­˜åŸå§‹è®­ç»ƒæ–¹æ³•
        self.original_train = self.student.train

        def correct_train_wrapper(**kwargs):
            return self._correct_training(**kwargs)

        # æ›¿æ¢è®­ç»ƒæ–¹æ³•
        self.student.train = correct_train_wrapper
        logging.info("âœ… æ­£ç¡®å›è°ƒè®¾ç½®å®Œæˆ")

    def _correct_training(self, **kwargs):
        """æ­£ç¡®è®­ç»ƒ"""
        logging.info("ğŸš€ å¼€å§‹æ­£ç¡®çš„è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒ...")

        # è®¾ç½®æ­£ç¡®å›è°ƒ
        self._setup_correct_training_callbacks()

        # ä½¿ç”¨æ­£ç¡®çš„è®­ç»ƒé…ç½®
        correct_kwargs = self._get_correct_training_config(kwargs)

        # ä½¿ç”¨åŸå§‹è®­ç»ƒ
        results = self.original_train(**correct_kwargs)

        # æœ€ç»ˆæŠ¥å‘Š
        self._final_correct_report()

        return results

    def _get_correct_training_config(self, kwargs):
        """è·å–æ­£ç¡®çš„è®­ç»ƒé…ç½® - ä½¿ç”¨æœ‰æ•ˆçš„YOLOå‚æ•°"""
        # ä½¿ç”¨YOLOæ”¯æŒçš„æœ‰æ•ˆå‚æ•°
        correct_config = {
            'data': self.data_config,
            'epochs': kwargs.get('epochs', 100),
            'imgsz': kwargs.get('imgsz', 640),
            'batch': kwargs.get('batch', 16),
            'device': self.device,
            'workers': kwargs.get('workers', 4),
            'lr0': 0.001,  # é™ä½å­¦ä¹ ç‡ä»¥æé«˜ç¨³å®šæ€§
            'amp': False,  # å…³é—­æ··åˆç²¾åº¦è®­ç»ƒ
            'verbose': True,
            'project': kwargs.get('project', 'runs/correct_ebbinghaus'),
            'name': kwargs.get('name', f'correct_{int(time.time())}'),
            # åªä½¿ç”¨YOLOæ”¯æŒçš„æœ‰æ•ˆå‚æ•°
        }

        # æ·»åŠ å…¶ä»–æœ‰æ•ˆå‚æ•°
        valid_args = ['patience', 'save_period', 'seed', 'cos_lr', 'label_smoothing']
        for arg in valid_args:
            if arg in kwargs:
                correct_config[arg] = kwargs[arg]

        logging.info(f"ğŸ”§ æ­£ç¡®è®­ç»ƒé…ç½®: lr0={correct_config['lr0']}, amp={correct_config['amp']}")
        return correct_config

    def _setup_correct_training_callbacks(self):
        """è®¾ç½®æ­£ç¡®è®­ç»ƒå›è°ƒ"""
        logging.info("ğŸ”§ è®¾ç½®æ­£ç¡®è®­ç»ƒå›è°ƒ...")

        if not hasattr(self.student, 'callbacks'):
            self.student.callbacks = {}

        def on_train_batch_start(trainer):
            """æ­£ç¡®æ‰¹æ¬¡å¼€å§‹å›è°ƒ"""
            self.step_count += 1

            try:
                # æ£€æŸ¥NaNçŠ¶æ€
                if self.nan_detected:
                    self.nan_recovery_steps += 1
                    if self.nan_recovery_steps >= 100:
                        self.nan_detected = False
                        self.nan_recovery_steps = 0
                        logging.info("ğŸ”„ å·²ä»NaNçŠ¶æ€æ¢å¤")

                # æå–æ‰¹æ¬¡æ•°æ®
                batch = self._get_correct_batch_data(trainer)
                if batch is not None:
                    self.current_batch = batch

                    if self.step_count % 200 == 0:
                        imgs = batch.get('img') if hasattr(batch, 'get') else None
                        if imgs is not None and hasattr(imgs, 'shape'):
                            logging.info(f"âœ… æ­¥éª¤{self.step_count}: è·å–çœŸå®æ‰¹æ¬¡æ•°æ®, å½¢çŠ¶: {imgs.shape}")
                else:
                    if self.step_count % 200 == 0:
                        logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: ä½¿ç”¨æ­£ç¡®è™šæ‹Ÿæ•°æ®")
                    self.current_batch = self._create_correct_virtual_batch()

            except Exception as e:
                if self.step_count % 200 == 0:
                    logging.warning(f"âŒ æ­¥éª¤{self.step_count}æ‰¹æ¬¡å¼€å§‹å›è°ƒå¤±è´¥: {e}")

        def on_train_batch_end(trainer):
            """æ­£ç¡®æ‰¹æ¬¡ç»“æŸå›è°ƒ"""
            try:
                # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                if hasattr(trainer, 'loss') and trainer.loss is not None:
                    try:
                        if hasattr(trainer.loss, 'item'):
                            loss_value = trainer.loss.item()
                        else:
                            loss_value = float(trainer.loss)
                        if math.isnan(loss_value) or math.isinf(loss_value):
                            if not self.nan_detected:
                                logging.warning("âš ï¸ æ£€æµ‹åˆ°NaNæŸå¤±ï¼Œå¯ç”¨æ­£ç¡®æ¨¡å¼")
                                self.nan_detected = True
                                self.stable_mode = True
                            return  # è·³è¿‡è’¸é¦
                    except:
                        pass

                self._apply_correct_distillation(trainer)
            except Exception as e:
                if self.step_count % 200 == 0:
                    logging.warning(f"âŒ æ­¥éª¤{self.step_count}è’¸é¦å¤±è´¥: {e}")

        def on_train_epoch_end(trainer):
            self.epoch_count += 1
            self._perform_correct_review()
            self._epochly_correct_report()

        # æ³¨å†Œå›è°ƒ
        if 'on_train_batch_start' not in self.student.callbacks:
            self.student.callbacks['on_train_batch_start'] = []
        if 'on_train_batch_end' not in self.student.callbacks:
            self.student.callbacks['on_train_batch_end'] = []
        if 'on_train_epoch_end' not in self.student.callbacks:
            self.student.callbacks['on_train_epoch_end'] = []

        self.student.callbacks['on_train_batch_start'].append(on_train_batch_start)
        self.student.callbacks['on_train_batch_end'].append(on_train_batch_end)
        self.student.callbacks['on_train_epoch_end'].append(on_train_epoch_end)

        logging.info("âœ… æ­£ç¡®è®­ç»ƒå›è°ƒè®¾ç½®å®Œæˆ")

    def _get_correct_batch_data(self, trainer):
        """è·å–æ­£ç¡®æ‰¹æ¬¡æ•°æ®"""
        try:
            # æ–¹æ³•1: æ£€æŸ¥trainer.batch
            if hasattr(trainer, 'batch') and trainer.batch is not None:
                return trainer.batch

            # æ–¹æ³•2: æ£€æŸ¥å…¶ä»–å¯èƒ½å±æ€§
            for attr_name in ['batch_data', 'current_batch', 'data_batch']:
                if hasattr(trainer, attr_name):
                    batch = getattr(trainer, attr_name)
                    if batch is not None:
                        return batch

            return None
        except:
            return None

    def _create_correct_virtual_batch(self):
        """åˆ›å»ºæ­£ç¡®è™šæ‹Ÿæ‰¹æ¬¡"""
        try:
            batch_size = 8
            img_size = 640

            # ä½¿ç”¨æ›´ç¨³å®šçš„éšæœºæ•°ç”Ÿæˆ
            virtual_batch = {
                'img': torch.randn(batch_size, 3, img_size, img_size, device=self.device) * 0.1 + 0.5,
                'cls': torch.randint(0, 1, (batch_size,), device=self.device),
                'bbox': torch.rand(batch_size, 4, device=self.device) * 0.8 + 0.1
            }
            return virtual_batch
        except:
            return None

    def _apply_correct_distillation(self, trainer):
        """åº”ç”¨æ­£ç¡®è’¸é¦"""
        if not hasattr(self, 'current_batch') or self.current_batch is None:
            if self.step_count % 200 == 0:
                logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: æ— æ‰¹æ¬¡æ•°æ®ï¼Œè·³è¿‡è’¸é¦")
            return

        if self.nan_detected and self.stable_mode:
            if self.step_count % 100 == 0:
                logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: NaNæ¢å¤æ¨¡å¼ï¼Œè·³è¿‡è’¸é¦")
            return

        batch = self.current_batch

        try:
            # éªŒè¯æ‰¹æ¬¡æ•°æ®
            if not self._validate_correct_batch(batch):
                if self.step_count % 200 == 0:
                    logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: æ‰¹æ¬¡æ•°æ®æ— æ•ˆï¼Œä½¿ç”¨æ­£ç¡®è™šæ‹Ÿæ•°æ®")
                batch = self._create_correct_virtual_batch()
                if batch is None:
                    return

            # è·å–å›¾åƒæ•°æ®
            imgs = batch.get('img') if hasattr(batch, 'get') else None
            if imgs is None:
                if self.step_count % 200 == 0:
                    logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: æ— å›¾åƒæ•°æ®ï¼Œä½¿ç”¨æ­£ç¡®è™šæ‹Ÿå›¾åƒ")
                imgs = torch.randn(8, 3, 640, 640, device=self.device) * 0.1 + 0.5

            if self.step_count % 200 == 0:
                logging.info(f"ğŸ§  æ­¥éª¤{self.step_count}: åº”ç”¨æ­£ç¡®è’¸é¦, å›¾åƒå½¢çŠ¶: {imgs.shape}")

            # è®°å½•åŸå§‹æŸå¤±
            original_loss = 0.0
            if hasattr(trainer, 'loss') and trainer.loss is not None:
                try:
                    if hasattr(trainer.loss, 'item'):
                        original_loss = trainer.loss.item()
                    else:
                        original_loss = float(trainer.loss)
                except:
                    original_loss = 0.0

            # å‡†å¤‡å›¾åƒ
            imgs = imgs.to(self.device).float()

            # æ£€æŸ¥å›¾åƒæ•°æ®ç¨³å®šæ€§
            if torch.isnan(imgs).any() or torch.isinf(imgs).any():
                logging.warning("âš ï¸ å›¾åƒæ•°æ®åŒ…å«NaNæˆ–Infï¼Œä½¿ç”¨æ­£ç¡®è™šæ‹Ÿæ•°æ®")
                imgs = torch.randn(8, 3, 640, 640, device=self.device) * 0.1 + 0.5

            if imgs.max() > 1.0:
                imgs = imgs / 255.0

            # æ•™å¸ˆé¢„æµ‹
            with torch.no_grad():
                try:
                    teacher_outputs = self.teacher.model(imgs)
                except Exception as e:
                    logging.warning(f"âŒ æ•™å¸ˆé¢„æµ‹å¤±è´¥: {e}")
                    return

            # å­¦ç”Ÿé¢„æµ‹
            try:
                student_outputs = self.student.model(imgs)
            except Exception as e:
                logging.warning(f"âŒ å­¦ç”Ÿé¢„æµ‹å¤±è´¥: {e}")
                return

            # è®¡ç®—æ­£ç¡®è’¸é¦æŸå¤±
            distill_loss = self._compute_correct_distillation_loss(student_outputs, teacher_outputs, imgs)

            # æ£€æŸ¥è’¸é¦æŸå¤±ç¨³å®šæ€§
            if not self._check_tensor_stability(distill_loss, 'è’¸é¦æŸå¤±'):
                logging.warning("âš ï¸ è’¸é¦æŸå¤±ä¸ç¨³å®šï¼Œä½¿ç”¨é»˜è®¤å€¼")
                distill_loss = torch.tensor(0.05, device=self.device, requires_grad=True)

            # åº”ç”¨è’¸é¦æŸå¤±
            distill_weight = 0.1 if self.stable_mode else 0.3
            weighted_distill = distill_weight * distill_loss

            if hasattr(trainer, 'loss') and trainer.loss is not None:
                # æ£€æŸ¥å½“å‰æŸå¤±ç¨³å®šæ€§
                if not self._check_loss_stability(trainer.loss):
                    logging.warning("âš ï¸ è®­ç»ƒæŸå¤±ä¸ç¨³å®šï¼Œé‡ç½®ä¸ºè’¸é¦æŸå¤±")
                    trainer.loss = weighted_distill
                else:
                    trainer.loss = trainer.loss + weighted_distill
            else:
                trainer.loss = weighted_distill

            # å®‰å…¨è·å–è’¸é¦æŸå¤±å€¼
            distill_loss_value = 0.0
            try:
                if hasattr(distill_loss, 'item'):
                    distill_loss_value = distill_loss.item()
                else:
                    distill_loss_value = float(distill_loss)
            except:
                distill_loss_value = 0.05

            # æ›´æ–°æ­£ç¡®è®°å¿†ç³»ç»Ÿ
            self._update_correct_memory_system(batch, distill_loss_value, original_loss)

            self.distill_applied += 1

            # è®°å½•æ—¥å¿—
            if self.step_count % 200 == 0:
                new_loss = 0.0
                if hasattr(trainer, 'loss') and trainer.loss is not None:
                    try:
                        if hasattr(trainer.loss, 'item'):
                            new_loss = trainer.loss.item()
                        else:
                            new_loss = float(trainer.loss)
                    except:
                        new_loss = 0.0

                memory_report = self.memory_model.get_memory_report()

                logging.info("ğŸ§  === æ­£ç¡®è’¸é¦æŠ¥å‘Š ===")
                logging.info(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {self.step_count}")
                logging.info(f"ğŸ’§ æŸå¤±å˜åŒ–: {original_loss:.4f} -> {new_loss:.4f}")
                logging.info(f"ğŸ”¥ è’¸é¦æŸå¤±: {distill_loss_value:.6f}")
                logging.info(f"ğŸ¯ å­¦ä¹ è¿›åº¦: {memory_report['learning_progress']:.1%}")
                logging.info(f"ğŸ“š è·Ÿè¸ªæ ·æœ¬: {memory_report['total_samples']}ä¸ª")
                logging.info(f"ğŸ’¾ å¹³å‡è®°å¿†å¼ºåº¦: {memory_report['avg_intensity']:.3f}")
                logging.info(f"ğŸ“ˆ è®°å¿†è¶‹åŠ¿: {memory_report['trend']}")
                logging.info(f"âœ… è’¸é¦åº”ç”¨æ¬¡æ•°: {self.distill_applied}")
                if self.stable_mode:
                    logging.info("ğŸ›¡ï¸ ç¨³å®šæ¨¡å¼: å·²å¯ç”¨")

        except Exception as e:
            if self.step_count % 200 == 0:
                logging.warning(f"âŒ æ­¥éª¤{self.step_count}è’¸é¦å¤±è´¥: {e}")

    def _compute_correct_distillation_loss(self, student_out, teacher_out, imgs):
        """è®¡ç®—æ­£ç¡®è’¸é¦æŸå¤±"""
        try:
            # ç®€åŒ–ä½†ç¨³å®šçš„è’¸é¦æŸå¤±è®¡ç®—
            total_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
            num_losses = 0

            # æå–ç‰¹å¾
            s_features = self._extract_correct_features(student_out, 'å­¦ç”Ÿ')
            t_features = self._extract_correct_features(teacher_out, 'æ•™å¸ˆ')

            if not s_features or not t_features:
                if self.step_count % 500 == 0:
                    logging.info("ğŸ”„ æ— æ³•æå–ç‰¹å¾ï¼Œä½¿ç”¨æ­£ç¡®é»˜è®¤æŸå¤±")
                return torch.tensor(0.05, device=self.device, requires_grad=True)

            # å¯¹æ¯ä¸ªç‰¹å¾å±‚è®¡ç®—æŸå¤±
            for i, (s_feat, t_feat) in enumerate(zip(s_features, t_features)):
                if s_feat is None or t_feat is None:
                    continue

                # ç¨³å®šåŒ–ç‰¹å¾å¼ é‡
                s_feat = self._stabilize_tensor(s_feat)
                t_feat = self._stabilize_tensor(t_feat)

                if s_feat is None or t_feat is None:
                    continue

                # ç¡®ä¿å½¢çŠ¶åŒ¹é…
                if s_feat.shape != t_feat.shape:
                    try:
                        # ä½¿ç”¨æ’å€¼è°ƒæ•´å­¦ç”Ÿç‰¹å¾å›¾å¤§å°
                        s_feat = F.interpolate(s_feat, size=t_feat.shape[2:], mode='bilinear', align_corners=False)
                    except Exception as e:
                        if self.step_count % 500 == 0:
                            logging.warning(f"âŒ ç‰¹å¾{i}æ’å€¼å¤±è´¥: {e}")
                        continue

                # è®¡ç®—MSEæŸå¤±
                try:
                    layer_loss = F.mse_loss(s_feat, t_feat) * 0.1

                    # æ£€æŸ¥æŸå¤±ç¨³å®šæ€§
                    if not self._check_tensor_stability(layer_loss, f"å±‚æŸå¤±{i}"):
                        continue

                    total_loss = total_loss + layer_loss
                    num_losses += 1

                except Exception as e:
                    if self.step_count % 500 == 0:
                        logging.warning(f"âŒ ç‰¹å¾{i}æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                    continue

            if num_losses > 0:
                avg_loss = total_loss / num_losses
                if self.step_count % 500 == 0:
                    logging.info(f"ğŸ¯ æ­£ç¡®ç‰¹å¾è’¸é¦æŸå¤±: {avg_loss.item():.6f} (åŸºäº{num_losses}ä¸ªç‰¹å¾å±‚)")
                return avg_loss
            else:
                if self.step_count % 500 == 0:
                    logging.info("ğŸ”„ æ‰€æœ‰ç‰¹å¾å±‚æŸå¤±è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨æ­£ç¡®é»˜è®¤æŸå¤±")
                return torch.tensor(0.05, device=self.device, requires_grad=True)

        except Exception as e:
            logging.warning(f"âŒ æ­£ç¡®è’¸é¦æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            return torch.tensor(0.05, device=self.device, requires_grad=True)

    def _extract_correct_features(self, model_output, model_type):
        """æå–æ­£ç¡®ç‰¹å¾"""
        features = []

        try:
            if model_output is None:
                return features

            # æ–¹æ³•1: ç›´æ¥æ˜¯ç‰¹å¾å›¾
            if isinstance(model_output, torch.Tensor):
                features.append(model_output)
                return features

            # æ–¹æ³•2: åˆ—è¡¨æˆ–å…ƒç»„
            if isinstance(model_output, (list, tuple)):
                for i, output in enumerate(model_output):
                    if isinstance(output, torch.Tensor):
                        features.append(output)
                    elif hasattr(output, 'detach'):
                        try:
                            output_tensor = output.detach()
                            features.append(output_tensor)
                        except:
                            pass

            # æ–¹æ³•3: å­—å…¸å½¢å¼
            if isinstance(model_output, dict):
                for key, value in model_output.items():
                    if isinstance(value, torch.Tensor):
                        features.append(value)

            # ç¨³å®šåŒ–æ‰€æœ‰ç‰¹å¾
            stable_features = []
            for feat in features:
                stable_feat = self._stabilize_tensor(feat)
                if stable_feat is not None:
                    stable_features.append(stable_feat)

            if self.step_count % 1000 == 0 and stable_features:
                logging.info(f"ğŸ” {model_type}ç‰¹å¾æå–: æ‰¾åˆ°{len(stable_features)}ä¸ªç¨³å®šç‰¹å¾")

            return stable_features

        except Exception as e:
            if self.step_count % 500 == 0:
                logging.warning(f"âŒ {model_type}ç‰¹å¾æå–å¤±è´¥: {e}")
            return []

    def _stabilize_tensor(self, tensor):
        """ç¨³å®šåŒ–å¼ é‡"""
        try:
            if tensor is None:
                return None

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼ é‡
            if not isinstance(tensor, torch.Tensor):
                try:
                    tensor = torch.tensor(tensor, device=self.device)
                except:
                    return None

            # æ£€æŸ¥NaNå’ŒInf
            if torch.isnan(tensor).any() or torch.isinf(tensor).any():
                # æ›¿æ¢NaNå’ŒInfä¸º0
                tensor = torch.where(torch.isnan(tensor), torch.zeros_like(tensor), tensor)
                tensor = torch.where(torch.isinf(tensor), torch.zeros_like(tensor), tensor)

            return tensor

        except Exception as e:
            if self.step_count % 500 == 0:
                logging.warning(f"âŒ å¼ é‡ç¨³å®šåŒ–å¤±è´¥: {e}")
            return None

    def _check_tensor_stability(self, tensor, tensor_name):
        """æ£€æŸ¥å¼ é‡ç¨³å®šæ€§"""
        try:
            if tensor is None:
                return False

            if not isinstance(tensor, torch.Tensor):
                return False

            # æ£€æŸ¥NaNå’ŒInf
            if torch.isnan(tensor).any():
                logging.warning(f"âš ï¸ {tensor_name}åŒ…å«NaN")
                return False

            if torch.isinf(tensor).any():
                logging.warning(f"âš ï¸ {tensor_name}åŒ…å«Inf")
                return False

            return True

        except:
            return False

    def _check_loss_stability(self, loss):
        """æ£€æŸ¥æŸå¤±ç¨³å®šæ€§"""
        try:
            if loss is None:
                return False

            if hasattr(loss, 'item'):
                loss_value = loss.item()
            else:
                loss_value = float(loss)

            if math.isnan(loss_value) or math.isinf(loss_value):
                return False

            if abs(loss_value) > 1e6:
                return False

            return True

        except:
            return False

    def _validate_correct_batch(self, batch):
        """éªŒè¯æ­£ç¡®æ‰¹æ¬¡"""
        try:
            if batch is None:
                return False
            if not hasattr(batch, 'get'):
                return False
            if batch.get('img') is None:
                return False
            return True
        except:
            return False

    def _update_correct_memory_system(self, batch, distill_loss_value, original_loss):
        """æ›´æ–°æ­£ç¡®è®°å¿†ç³»ç»Ÿ"""
        try:
            if batch is None:
                return

            # è·å–æ‰¹æ¬¡å¤§å°
            batch_size = 1
            if hasattr(batch, 'get'):
                imgs = batch.get('img')
                if imgs is not None and hasattr(imgs, 'shape') and len(imgs.shape) > 0:
                    batch_size = imgs.shape[0]

            for i in range(batch_size):
                sample_id = self._get_correct_sample_id(batch, i)

                # è®¡ç®—æ­£ç¡®å­¦ä¹ å¢ç›Š
                learning_gain = self._compute_correct_learning_gain(distill_loss_value, original_loss)

                # è®¡ç®—æ ·æœ¬éš¾åº¦
                difficulty = self.memory_model.compute_sample_difficulty(batch, i)

                # æ›´æ–°æ ·æœ¬è®°å¿†
                self.memory_model.update_sample_memory(
                    sample_id, learning_gain, self.step_count, difficulty, is_review=False
                )

            if self.step_count % 500 == 0:
                memory_report = self.memory_model.get_memory_report()
                logging.info(
                    f"ğŸ’¾ æ­£ç¡®è®°å¿†æ›´æ–°: æ‰¹æ¬¡å¤§å°={batch_size}, æ ·æœ¬æ•°={memory_report['total_samples']}, å­¦ä¹ è¿›åº¦={memory_report['learning_progress']:.1%}")

        except Exception as e:
            if self.step_count % 500 == 0:
                logging.warning(f"âŒ æ­£ç¡®è®°å¿†æ›´æ–°å¤±è´¥: {e}")

    def _compute_correct_learning_gain(self, distill_loss, original_loss):
        """è®¡ç®—æ­£ç¡®å­¦ä¹ å¢ç›Š"""
        try:
            # ç¡®ä¿è’¸é¦æŸå¤±ä¸ºæ­£
            distill_loss = max(0.0, distill_loss)

            # åŸºäºè’¸é¦æŸå¤±çš„è®¡ç®—
            if distill_loss <= 0.01:
                base_gain = 0.15
            elif distill_loss <= 0.05:
                base_gain = 0.08
            else:
                base_gain = 0.03

            # ç¨³å®šæ¨¡å¼ä¸‹é™ä½å¢ç›Š
            if self.stable_mode:
                base_gain = base_gain * 0.5

            # é™åˆ¶å¢ç›ŠèŒƒå›´
            final_gain = max(0.01, min(0.2, base_gain))

            if self.step_count % 1000 == 0:
                logging.info(
                    f"ğŸ¯ æ­£ç¡®å­¦ä¹ å¢ç›Š: è’¸é¦æŸå¤±={distill_loss:.4f}, å¢ç›Š={final_gain:.4f}, ç¨³å®šæ¨¡å¼={self.stable_mode}")

            return final_gain

        except Exception as e:
            if self.step_count % 500 == 0:
                logging.warning(f"âŒ æ­£ç¡®å­¦ä¹ å¢ç›Šè®¡ç®—å¤±è´¥: {e}")
            return 0.05

    def _get_correct_sample_id(self, batch, index):
        """è·å–æ­£ç¡®æ ·æœ¬ID"""
        try:
            if not hasattr(batch, 'get'):
                return f"batch_{index}_{self.step_count}"

            imgs = batch.get('img')
            if imgs is None:
                return f"no_img_{index}_{self.step_count}"

            if hasattr(imgs, 'shape') and index < imgs.shape[0]:
                try:
                    if hasattr(imgs, '__getitem__'):
                        img_tensor = imgs[index]
                    else:
                        img_tensor = imgs

                    if hasattr(img_tensor, 'mean') and hasattr(img_tensor, 'std'):
                        img_mean = img_tensor.mean().item()
                        img_std = img_tensor.std().item()

                        # ä½¿ç”¨æ›´ç¨³å®šçš„IDç”Ÿæˆ
                        id_string = f"{index}_{self.step_count}_{img_mean:.6f}"
                        img_hash = hashlib.md5(id_string.encode()).hexdigest()[:8]
                        return f"img_{img_hash}"
                except:
                    pass

            return f"sample_{index}_{self.step_count}"
        except:
            return f"error_{index}_{self.step_count}"

    def _perform_correct_review(self):
        """æ‰§è¡Œæ­£ç¡®å¤ä¹ """
        logging.info("ğŸ“– æ‰§è¡Œæ­£ç¡®è‡ªé€‚åº”å¤ä¹ è°ƒåº¦...")

        # å®‰æ’å¤ä¹ 
        review_samples = self.review_scheduler.schedule_reviews(
            self.step_count, review_ratio=0.3
        )

        if review_samples:
            logging.info(f"âœ… å®‰æ’äº†{len(review_samples)}ä¸ªæ ·æœ¬çš„å¤ä¹ ")
        else:
            logging.info("ğŸ”„ æš‚æ— éœ€è¦å¤ä¹ çš„æ ·æœ¬")

    def _epochly_correct_report(self):
        """æ­£ç¡®å‘¨æœŸæŠ¥å‘Š"""
        memory_report = self.memory_model.get_memory_report()
        scheduling_report = self.review_scheduler.get_scheduling_report()

        logging.info("ğŸ“ˆ === æ­£ç¡®å‘¨æœŸæŠ¥å‘Š ===")
        logging.info(f"ğŸ“… è®­ç»ƒå‘¨æœŸ: {self.epoch_count}")
        logging.info(f"ğŸ”„ æ€»è®­ç»ƒæ­¥æ•°: {self.step_count}")
        logging.info(f"âœ… è’¸é¦åº”ç”¨æ¬¡æ•°: {self.distill_applied}")
        logging.info(f"ğŸ¯ å­¦ä¹ è¿›åº¦: {memory_report['learning_progress']:.1%}")
        logging.info(f"ğŸ“š è·Ÿè¸ªæ ·æœ¬: {memory_report['total_samples']}ä¸ª")
        logging.info(f"ğŸ’¾ å¹³å‡è®°å¿†å¼ºåº¦: {memory_report['avg_intensity']:.3f}")
        logging.info(f"ğŸ“ˆ è®°å¿†è¶‹åŠ¿: {memory_report['trend']}")
        logging.info(f"ğŸ“… å¤ä¹ å®‰æ’: {scheduling_report['scheduled_reviews']}æ¬¡")

        # NaNçŠ¶æ€æŠ¥å‘Š
        if self.nan_detected:
            logging.info("âš ï¸  NaNæ£€æµ‹: ç³»ç»Ÿå¤„äºç¨³å®šæ¢å¤æ¨¡å¼")
        elif self.stable_mode:
            logging.info("ğŸ›¡ï¸  ç¨³å®šæ¨¡å¼: å·²å¯ç”¨ï¼Œè®­ç»ƒæ›´åŠ ä¿å®ˆ")

        logging.info("=" * 50)

    def _final_correct_report(self):
        """æ­£ç¡®æœ€ç»ˆæŠ¥å‘Š"""
        training_time = time.time() - self.start_time
        memory_report = self.memory_model.get_memory_report()
        scheduling_report = self.review_scheduler.get_scheduling_report()

        logging.info("ğŸ‰ === æ­£ç¡®æœ€ç»ˆæŠ¥å‘Š ===")
        logging.info(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time:.0f}ç§’")
        logging.info(f"ğŸ”„ æ€»è®­ç»ƒæ­¥æ•°: {self.step_count}")
        logging.info(f"ğŸ“… æ€»è®­ç»ƒå‘¨æœŸ: {self.epoch_count}")
        logging.info(f"âœ… è’¸é¦åº”ç”¨æ¬¡æ•°: {self.distill_applied}")
        logging.info(f"ğŸ¯ æœ€ç»ˆå­¦ä¹ è¿›åº¦: {memory_report['learning_progress']:.1%}")
        logging.info(f"ğŸ“š æ€»è·Ÿè¸ªæ ·æœ¬: {memory_report['total_samples']}ä¸ª")
        logging.info(f"ğŸ’¾ æœ€ç»ˆè®°å¿†å¼ºåº¦: {memory_report['avg_intensity']:.3f}")
        logging.info(f"ğŸ“ˆ æœ€ç»ˆè®°å¿†è¶‹åŠ¿: {memory_report['trend']}")
        logging.info(f"ğŸ“… æ€»å¤ä¹ å®‰æ’: {scheduling_report['total_reviews']}æ¬¡")

        # ç³»ç»Ÿç¨³å®šæ€§è¯„ä¼°
        if not self.nan_detected and memory_report['learning_progress'] > 0:
            logging.info("âœ… æ­£ç¡®è’¸é¦ç³»ç»Ÿå·¥ä½œæ­£å¸¸!")
        elif self.nan_detected:
            logging.info("âš ï¸  ç³»ç»Ÿæ£€æµ‹åˆ°NaNï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        else:
            logging.info("ğŸ”„ ç³»ç»Ÿç¨³å®šä½†å­¦ä¹ è¿›åº¦è¾ƒä½")

        logging.info("=" * 50)

    def train(self, epochs=100, imgsz=640, batch=16, **kwargs):
        """è®­ç»ƒå…¥å£"""
        logging.info("ğŸš€ å¼€å§‹æ­£ç¡®çš„è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒ...")

        # ä½¿ç”¨æ­£ç¡®çš„è®­ç»ƒé…ç½®
        correct_config = {
            'data': self.data_config,
            'epochs': epochs,
            'imgsz': imgsz,
            'batch': batch,
            'device': self.device,
            'workers': kwargs.get('workers', 4),
            'lr0': 0.001,  # é™ä½å­¦ä¹ ç‡
            'amp': False,  # å…³é—­æ··åˆç²¾åº¦
            'verbose': True,
            'project': kwargs.get('project', 'runs/correct_ebbinghaus'),
            'name': kwargs.get('name', f'correct_{int(time.time())}'),
        }

        try:
            results = self.student.train(**correct_config)
            logging.info("âœ… æ­£ç¡®çš„è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒå®Œæˆ")
            return results

        except Exception as e:
            logging.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise


class CorrectMemoryModel:
    """æ­£ç¡®è®°å¿†æ¨¡å‹"""

    def __init__(self):
        self.sample_memory_db = {}
        self.sample_difficulty_db = {}
        self.global_memory_intensity = 0.5
        self.history_intensities = deque(maxlen=100)

        # æ­£ç¡®å‚æ•°
        self.base_forgetting_rate = 0.998
        self.learning_threshold = 0.5
        self.forgetting_threshold = 0.3

        # ç»Ÿè®¡
        self.total_samples = 0
        self.learned_samples = 0

        logging.info("ğŸ§ âœ… æ­£ç¡®è®°å¿†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def compute_sample_difficulty(self, batch, index):
        """è®¡ç®—æ ·æœ¬éš¾åº¦"""
        return 1.0  # ç®€åŒ–å®ç°

    def update_sample_memory(self, sample_id, learning_gain, current_step, difficulty, is_review=False):
        """æ›´æ–°æ ·æœ¬è®°å¿†"""
        if sample_id not in self.sample_memory_db:
            self.sample_memory_db[sample_id] = {
                'strength': 0.5,
                'last_learning_time': current_step,
                'learning_count': 0
            }
            self.total_samples += 1

        record = self.sample_memory_db[sample_id]

        # è®°å¿†è¡°å‡
        time_gap = max(1, current_step - record['last_learning_time'])
        time_decay = math.exp(-time_gap / self.base_forgetting_rate)
        record['strength'] = record['strength'] * time_decay

        # è®°å¿†å¼ºåŒ–
        record['strength'] = min(1.0, record['strength'] + learning_gain * 0.1)
        record['last_learning_time'] = current_step
        record['learning_count'] += 1

        # æ£€æŸ¥å­¦ä¹ çŠ¶æ€
        if record['strength'] >= self.learning_threshold and record['learning_count'] == 1:
            self.learned_samples += 1

        # æ›´æ–°å…¨å±€è®°å¿†å¼ºåº¦
        self._update_global_memory()

        return record['strength']

    def _update_global_memory(self):
        """æ›´æ–°å…¨å±€è®°å¿†å¼ºåº¦"""
        if not self.sample_memory_db:
            self.global_memory_intensity = 0.5
            return

        total_intensity = sum(record['strength'] for record in self.sample_memory_db.values())
        self.global_memory_intensity = total_intensity / len(self.sample_memory_db)

        # è®°å½•å†å²å¼ºåº¦
        self.history_intensities.append(self.global_memory_intensity)

    def get_memory_report(self):
        """è·å–è®°å¿†æŠ¥å‘Š"""
        if self.total_samples == 0:
            return {
                'total_samples': 0,
                'learned_samples': 0,
                'learning_progress': 0.0,
                'avg_intensity': 0.5,
                'trend': 'stable'
            }

        learning_progress = self.learned_samples / self.total_samples
        avg_intensity = self.global_memory_intensity

        # è®¡ç®—è¶‹åŠ¿
        trend = 'stable'
        if len(self.history_intensities) >= 10:
            recent = np.mean(list(self.history_intensities)[-5:])
            earlier = np.mean(list(self.history_intensities)[-10:-5])
            if recent > earlier + 0.01:
                trend = 'increasing'
            elif recent < earlier - 0.01:
                trend = 'decreasing'

        return {
            'total_samples': self.total_samples,
            'learned_samples': self.learned_samples,
            'learning_progress': learning_progress,
            'avg_intensity': avg_intensity,
            'trend': trend
        }


class CorrectReviewScheduler:
    """æ­£ç¡®å¤ä¹ è°ƒåº¦å™¨"""

    def __init__(self, memory_model):
        self.memory_model = memory_model
        self.review_count = 0

        logging.info("ğŸ“…âœ… æ­£ç¡®å¤ä¹ è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")

    def schedule_reviews(self, current_step, review_ratio=0.3):
        """å®‰æ’å¤ä¹ """
        if not self.memory_model.sample_memory_db:
            return []

        need_review = []
        for sample_id, memory_info in self.memory_model.sample_memory_db.items():
            if memory_info['strength'] < 0.4:
                need_review.append(sample_id)

        num_review = min(len(need_review), int(len(self.memory_model.sample_memory_db) * review_ratio))
        review_samples = need_review[:num_review]

        self.review_count += len(review_samples)

        return review_samples

    def get_scheduling_report(self):
        """è·å–è°ƒåº¦æŠ¥å‘Š"""
        return {
            'total_reviews': self.review_count,
            'scheduled_reviews': self.review_count
        }


def main():
    """ä¸»å‡½æ•°"""
    torch.manual_seed(42)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    print("=== æ­£ç¡®çš„è‰¾å®¾æµ©æ–¯è’¸é¦ç³»ç»Ÿ ===")
    print("ğŸš€ å¼€å§‹æ­£ç¡®ç‰ˆæœ¬è®­ç»ƒ...")

    try:
        trainer = CorrectEbbinghausDistillation(
            teacher_path="runs/segment/train2/weights/best.pt",
            student_config="yolo11n-seg.yaml",
            data_config="./datasets/crack-seg/data.yaml",
            device=device
        )

        results = trainer.train(
            epochs=100,
            imgsz=640,
            batch=8,
            workers=0,
            lr0=0.001,
            amp=False
        )

        logging.info("ğŸ‰ æ­£ç¡®çš„è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒå®Œæˆ!")
        return results

    except Exception as e:
        logging.error(f"âŒ æ­£ç¡®ç‰ˆæœ¬å¤±è´¥: {e}")
        return None


if __name__ == '__main__':
    main()