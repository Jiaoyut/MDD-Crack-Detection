import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
import logging
import time
import math
import hashlib
import numpy as np
from collections import defaultdict, deque
import cv2

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'


class EbbinghausDynamicDistillation:
    """è‰¾å®¾æµ©æ–¯åŠ¨æ€è’¸é¦ç³»ç»Ÿ - å®Œæ•´å®ç°æ‚¨æä¾›çš„ç†è®ºæ¡†æ¶"""

    def __init__(self, teacher_path, student_config, data_config, device='cuda'):
        self.device = device
        self.teacher_path = teacher_path
        self.student_config = student_config
        self.data_config = data_config

        # è®­ç»ƒçŠ¶æ€
        self.step_count = 0
        self.epoch_count = 0
        self.distill_applied = 0
        self.start_time = time.time()

        # è‰¾å®¾æµ©æ–¯è®°å¿†ç³»ç»Ÿ
        self.memory_model = EbbinghausMemoryModel()
        self.review_scheduler = AdaptiveReviewScheduler(self.memory_model)
        self.distillation_loss = MemoryAwareDistillationLoss(self.memory_model)

        self.setup_models()
        self.setup_ebbinghaus_callbacks()

        logging.info("ğŸ§  è‰¾å®¾æµ©æ–¯åŠ¨æ€è’¸é¦ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def setup_models(self):
        """è®¾ç½®æ¨¡å‹"""
        try:
            # æ•™å¸ˆæ¨¡å‹
            self.teacher = YOLO(self.teacher_path)
            self.teacher.model.to(self.device).eval()
            for p in self.teacher.model.parameters():
                p.requires_grad = False

            # å­¦ç”Ÿæ¨¡å‹
            self.student = YOLO(self.student_config)
            self.student.model.to(self.device)

            logging.info("âœ… æ¨¡å‹è®¾ç½®å®Œæˆ")
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            raise

    def setup_ebbinghaus_callbacks(self):
        """è®¾ç½®è‰¾å®¾æµ©æ–¯è’¸é¦å›è°ƒ"""
        logging.info("ğŸ”§ è®¾ç½®è‰¾å®¾æµ©æ–¯è’¸é¦å›è°ƒ...")

        # ä¿å­˜åŸå§‹è®­ç»ƒæ–¹æ³•
        self.original_train = self.student.train

        def ebbinghaus_train_wrapper(**kwargs):
            return self._ebbinghaus_training(**kwargs)

        # æ›¿æ¢è®­ç»ƒæ–¹æ³•
        self.student.train = ebbinghaus_train_wrapper
        logging.info("âœ… è‰¾å®¾æµ©æ–¯è’¸é¦å›è°ƒè®¾ç½®å®Œæˆ")

    def _ebbinghaus_training(self,  ** kwargs):
        """è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒ"""
        logging.info("ğŸš€ å¼€å§‹è‰¾å®¾æµ©æ–¯åŠ¨æ€è’¸é¦è®­ç»ƒ...")

        # è®¾ç½®è‰¾å®¾æµ©æ–¯å›è°ƒ
        self._setup_ebbinghaus_training_callbacks()

        # ä½¿ç”¨åŸå§‹è®­ç»ƒ
        results = self.original_train(**kwargs)

        # æœ€ç»ˆæŠ¥å‘Š
        self._final_ebbinghaus_report()

        return results

    def _setup_ebbinghaus_training_callbacks(self):
        """è®¾ç½®è‰¾å®¾æµ©æ–¯è®­ç»ƒå›è°ƒ"""
        logging.info("ğŸ”§ è®¾ç½®è‰¾å®¾æµ©æ–¯è®­ç»ƒå›è°ƒ...")

        if not hasattr(self.student, 'callbacks'):
            self.student.callbacks = {}

        # è‰¾å®¾æµ©æ–¯æ‰¹æ¬¡å¼€å§‹å›è°ƒ
        def on_train_batch_start(trainer):
            """è‰¾å®¾æµ©æ–¯æ‰¹æ¬¡å¼€å§‹å›è°ƒ"""
            self.step_count += 1

            try:
                # æå–æ‰¹æ¬¡æ•°æ®
                batch = self._extract_batch_from_trainer(trainer)
                if batch is not None:
                    # æ›´æ–°å½“å‰æ‰¹æ¬¡æ•°æ®
                    self.current_batch = batch

                    # è®°å½•æ‰¹æ¬¡ä¿¡æ¯
                    if self.step_count % 100 == 0:
                        imgs = batch.get('img') if hasattr(batch, 'get') else None
                        if imgs is not None and hasattr(imgs, 'shape'):
                            logging.info(f"âœ… æ­¥éª¤{self.step_count}: è·å–æ‰¹æ¬¡æ•°æ®, å½¢çŠ¶: {imgs.shape}")
                else:
                    if self.step_count % 100 == 0:
                        logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: ä½¿ç”¨è™šæ‹Ÿæ‰¹æ¬¡æ•°æ®")
                    self.current_batch = self._create_ebbinghaus_batch()

            except Exception as e:
                if self.step_count % 200 == 0:
                    logging.warning(f"âŒ æ­¥éª¤{self.step_count}æ‰¹æ¬¡å¼€å§‹å›è°ƒå¤±è´¥: {e}")

        # è‰¾å®¾æµ©æ–¯æ‰¹æ¬¡ç»“æŸå›è°ƒ
        def on_train_batch_end(trainer):
            """è‰¾å®¾æµ©æ–¯æ‰¹æ¬¡ç»“æŸå›è°ƒ"""
            try:
                self._apply_ebbinghaus_distillation(trainer)
            except Exception as e:
                if self.step_count % 200 == 0:
                    logging.warning(f"âŒ æ­¥éª¤{self.step_count}è‰¾å®¾æµ©æ–¯è’¸é¦å¤±è´¥: {e}")

        # è‰¾å®¾æµ©æ–¯å‘¨æœŸç»“æŸå›è°ƒ
        def on_train_epoch_end(trainer):
            self.epoch_count += 1
            self._perform_adaptive_review()
            self._epochly_ebbinghaus_report()

        # æ³¨å†Œå›è°ƒ
        if 'on_train_batch_start' not in self.student.callbacks:
            self.student.callbacks['on_train_batch_start'] = []
        if 'on_train_batch_end' not in self.student.callbacks:
            self.student.callbacks['on_train_batch_end'] = []
        if 'on_train_epoch_end' not in self.student.callbacks:
            self.student.callbacks['on_train_epoch_end'] = []

        self.student.callbacks['on_train_batch_start'].append(on_train_batch_start)
        self.student.callbacks['on_train_batch_end'].append(on_train_batch_end)
        self.student.callbacks['on_train_epoch_end'].append(on_train_epoch_end)

        logging.info("âœ… è‰¾å®¾æµ©æ–¯è®­ç»ƒå›è°ƒè®¾ç½®å®Œæˆ")

    def _extract_batch_from_trainer(self, trainer):
        """ä»traineræå–æ‰¹æ¬¡æ•°æ®"""
        try:
            # æ–¹æ³•1: æ£€æŸ¥trainer.batchå±æ€§
            if hasattr(trainer, 'batch') and trainer.batch is not None:
                return trainer.batch

            # æ–¹æ³•2: æ£€æŸ¥å…¶ä»–å±æ€§
            for attr_name in ['batch_data', 'current_batch', 'data_batch']:
                if hasattr(trainer, attr_name):
                    batch = getattr(trainer, attr_name)
                    if batch is not None:
                        return batch

            return None
        except:
            return None

    def _create_ebbinghaus_batch(self):
        """åˆ›å»ºè‰¾å®¾æµ©æ–¯è™šæ‹Ÿæ‰¹æ¬¡"""
        try:
            batch_size = 8
            img_size = 640

            virtual_batch = {
                'img': torch.randn(batch_size, 3, img_size, img_size, device=self.device),
                'cls': torch.randint(0, 1, (batch_size,), device=self.device),
                'bbox': torch.rand(batch_size, 4, device=self.device)
            }
            return virtual_batch
        except:
            return None

    def _apply_ebbinghaus_distillation(self, trainer):
        """åº”ç”¨è‰¾å®¾æµ©æ–¯è’¸é¦"""
        if not hasattr(self, 'current_batch') or self.current_batch is None:
            if self.step_count % 100 == 0:
                logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: æ— æ‰¹æ¬¡æ•°æ®ï¼Œè·³è¿‡è’¸é¦")
            return

        batch = self.current_batch

        try:
            # æ£€æŸ¥æ‰¹æ¬¡æ•°æ®
            if not self._validate_batch(batch):
                if self.step_count % 100 == 0:
                    logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: æ‰¹æ¬¡æ•°æ®æ— æ•ˆï¼Œä½¿ç”¨è™šæ‹Ÿæ•°æ®")
                batch = self._create_ebbinghaus_batch()
                if batch is None:
                    return

            # è·å–å›¾åƒæ•°æ®
            imgs = batch.get('img') if hasattr(batch, 'get') else None
            if imgs is None:
                if self.step_count % 100 == 0:
                    logging.info(f"ğŸ”„ æ­¥éª¤{self.step_count}: æ— å›¾åƒæ•°æ®ï¼Œä½¿ç”¨è™šæ‹Ÿå›¾åƒ")
                imgs = torch.randn(8, 3, 640, 640, device=self.device)

            if self.step_count % 100 == 0:
                logging.info(f"ğŸ§  æ­¥éª¤{self.step_count}: åº”ç”¨è‰¾å®¾æµ©æ–¯è’¸é¦, å›¾åƒå½¢çŠ¶: {imgs.shape}")

            # è®°å½•åŸå§‹æŸå¤±
            original_loss = 0.0
            if hasattr(trainer, 'loss') and trainer.loss is not None:
                try:
                    if hasattr(trainer.loss, 'item'):
                        original_loss = trainer.loss.item()
                    else:
                        original_loss = float(trainer.loss)
                except:
                    original_loss = 0.0

            # å‡†å¤‡å›¾åƒ
            imgs = imgs.to(self.device).float()
            if imgs.max() > 1.0:
                imgs = imgs / 255.0

            # æ•™å¸ˆé¢„æµ‹
            with torch.no_grad():
                teacher_outputs = self.teacher.model(imgs)

            # å­¦ç”Ÿé¢„æµ‹
            student_outputs = self.student.model(imgs)

            # è®¡ç®—è‰¾å®¾æµ©æ–¯è’¸é¦æŸå¤±
            distill_loss = self.distillation_loss.compute(
                student_outputs, teacher_outputs, batch, self.step_count
            )

            # åº”ç”¨è’¸é¦æŸå¤±
            distill_weight = 0.3
            weighted_distill = distill_weight * distill_loss

            if hasattr(trainer, 'loss') and trainer.loss is not None:
                trainer.loss = trainer.loss + weighted_distill
            else:
                trainer.loss = weighted_distill

            # å®‰å…¨è·å–è’¸é¦æŸå¤±å€¼
            distill_loss_value = 0.0
            try:
                if hasattr(distill_loss, 'item'):
                    distill_loss_value = distill_loss.item()
                else:
                    distill_loss_value = float(distill_loss)
            except:
                distill_loss_value = 0.05

            # æ›´æ–°è‰¾å®¾æµ©æ–¯è®°å¿†ç³»ç»Ÿ
            self._update_ebbinghaus_memory_system(batch, distill_loss_value)

            self.distill_applied += 1

            # è®°å½•æ—¥å¿—
            if self.step_count % 100 == 0:
                new_loss = 0.0
                if hasattr(trainer, 'loss') and trainer.loss is not None:
                    try:
                        if hasattr(trainer.loss, 'item'):
                            new_loss = trainer.loss.item()
                        else:
                            new_loss = float(trainer.loss)
                    except:
                        new_loss = 0.0

                memory_report = self.memory_model.get_memory_report()
                scheduling_report = self.review_scheduler.get_scheduling_report()

                logging.info("ğŸ§  === è‰¾å®¾æµ©æ–¯è’¸é¦æŠ¥å‘Š ===")
                logging.info(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {self.step_count}")
                logging.info(f"ğŸ’§ æŸå¤±å˜åŒ–: {original_loss:.4f} -> {new_loss:.4f}")
                logging.info(f"ğŸ”¥ è’¸é¦æŸå¤±: {distill_loss_value:.6f}")
                logging.info(f"ğŸ¯ å­¦ä¹ è¿›åº¦: {memory_report['learning_progress']:.1%}")
                logging.info(f"ğŸ“š è·Ÿè¸ªæ ·æœ¬: {memory_report['total_samples']}ä¸ª")
                logging.info(f"ğŸ’¾ å¹³å‡è®°å¿†å¼ºåº¦: {memory_report['avg_intensity']:.3f}")
                logging.info(f"ğŸ“… å¤ä¹ å®‰æ’: {scheduling_report['scheduled_reviews']}æ¬¡")
                logging.info(f"âœ… è’¸é¦åº”ç”¨æ¬¡æ•°: {self.distill_applied}")

        except Exception as e:
            if self.step_count % 200 == 0:
                logging.warning(f"âŒ æ­¥éª¤{self.step_count}è‰¾å®¾æµ©æ–¯è’¸é¦å¤±è´¥: {e}")

    def _validate_batch(self, batch):
        """éªŒè¯æ‰¹æ¬¡æ•°æ®"""
        try:
            if batch is None:
                return False

            if hasattr(batch, 'get'):
                imgs = batch.get('img')
                if imgs is not None and hasattr(imgs, 'shape'):
                    return True
            return False
        except:
            return False

    def _update_ebbinghaus_memory_system(self, batch, distill_loss_value):
        """æ›´æ–°è‰¾å®¾æµ©æ–¯è®°å¿†ç³»ç»Ÿ"""
        try:
            if batch is None:
                return

            # è·å–æ‰¹æ¬¡å¤§å°
            batch_size = 1
            if hasattr(batch, 'get'):
                imgs = batch.get('img')
                if imgs is not None and hasattr(imgs, 'shape') and len(imgs.shape) > 0:
                    batch_size = imgs.shape[0]

            for i in range(batch_size):
                sample_id = self._get_sample_id(batch, i)

                # è®¡ç®—æ ·æœ¬éš¾åº¦
                difficulty = self.memory_model.compute_sample_difficulty(batch, i)

                # æ›´æ–°æ ·æœ¬è®°å¿†
                learning_gain = max(0.0, min(0.2, -distill_loss_value * 5.0))
                self.memory_model.update_sample_memory(
                    sample_id, learning_gain, self.step_count, difficulty, is_review=False
                )

            if self.step_count % 200 == 0:
                memory_report = self.memory_model.get_memory_report()
                logging.info(f"ğŸ’¾ è‰¾å®¾æµ©æ–¯è®°å¿†æ›´æ–°: æ‰¹æ¬¡å¤§å°={batch_size}, æ ·æœ¬æ•°={memory_report['total_samples']}")

        except Exception as e:
            if self.step_count % 200 == 0:
                logging.warning(f"âŒ è‰¾å®¾æµ©æ–¯è®°å¿†æ›´æ–°å¤±è´¥: {e}")

    def _get_sample_id(self, batch, index):
        """è·å–æ ·æœ¬ID"""
        try:
            if not hasattr(batch, 'get'):
                return f"batch_{index}_{self.step_count}"

            # ä½¿ç”¨å›¾åƒæ•°æ®ç”ŸæˆID
            imgs = batch.get('img')
            if imgs is not None and hasattr(imgs, 'shape') and index < imgs.shape[0]:
                try:
                    if hasattr(imgs, '__getitem__'):
                        img_tensor = imgs[index]
                    else:
                        img_tensor = imgs

                    if hasattr(img_tensor, 'mean') and hasattr(img_tensor, 'std'):
                        img_mean = img_tensor.mean().item()
                        img_std = img_tensor.std().item()

                        norm_mean = round(img_mean, 6)
                        norm_std = round(img_std, 6)

                        id_string = f"{norm_mean:.6f}_{norm_std:.6f}"
                        img_hash = hashlib.md5(id_string.encode()).hexdigest()[:8]
                        return f"img_{img_hash}"
                except:
                    pass

            return f"sample_{index}_{self.step_count}"
        except:
            return f"error_{index}_{self.step_count}"

    def _perform_adaptive_review(self):
        """æ‰§è¡Œè‡ªé€‚åº”å¤ä¹ """
        logging.info("ğŸ“– æ‰§è¡Œè‰¾å®¾æµ©æ–¯è‡ªé€‚åº”å¤ä¹ è°ƒåº¦...")

        # å®‰æ’å¤ä¹ 
        review_samples = self.review_scheduler.schedule_reviews(
            self.step_count, review_ratio=0.3
        )

        if review_samples:
            logging.info(f"âœ… å®‰æ’äº†{len(review_samples)}ä¸ªæ ·æœ¬çš„å¤ä¹ ")
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½å¤ä¹ æ ·æœ¬è¿›è¡Œè®­ç»ƒ
        else:
            logging.info("ğŸ”„ æš‚æ— éœ€è¦å¤ä¹ çš„æ ·æœ¬")

    def _epochly_ebbinghaus_report(self):
        """è‰¾å®¾æµ©æ–¯å‘¨æœŸæŠ¥å‘Š"""
        memory_report = self.memory_model.get_memory_report()
        scheduling_report = self.review_scheduler.get_scheduling_report()

        logging.info("ğŸ“ˆ === è‰¾å®¾æµ©æ–¯å‘¨æœŸæŠ¥å‘Š ===")
        logging.info(f"ğŸ“… è®­ç»ƒå‘¨æœŸ: {self.epoch_count}")
        logging.info(f"ğŸ”„ æ€»è®­ç»ƒæ­¥æ•°: {self.step_count}")
        logging.info(f"âœ… è’¸é¦åº”ç”¨æ¬¡æ•°: {self.distill_applied}")
        logging.info(f"ğŸ¯ å­¦ä¹ è¿›åº¦: {memory_report['learning_progress']:.1%}")
        logging.info(f"ğŸ“š è·Ÿè¸ªæ ·æœ¬: {memory_report['total_samples']}ä¸ª")
        logging.info(f"ğŸ’¾ å¹³å‡è®°å¿†å¼ºåº¦: {memory_report['avg_intensity']:.3f}")
        logging.info(f"ğŸ“… å¤ä¹ å®‰æ’: {scheduling_report['scheduled_reviews']}æ¬¡")
        logging.info(f"ğŸ“Š è®°å¿†å¥åº·åº¦: {memory_report['health_score']:.1%}")

        # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å·¥ä½œ
        if memory_report['total_samples'] > 0:
            logging.info("âœ… è‰¾å®¾æµ©æ–¯è’¸é¦ç³»ç»Ÿå·¥ä½œæ­£å¸¸!")
        else:
            logging.info("ğŸ”„ è‰¾å®¾æµ©æ–¯ç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–...")

        logging.info("=" * 50)

    def _final_ebbinghaus_report(self):
        """æœ€ç»ˆè‰¾å®¾æµ©æ–¯æŠ¥å‘Š"""
        training_time = time.time() - self.start_time
        memory_report = self.memory_model.get_memory_report()
        scheduling_report = self.review_scheduler.get_scheduling_report()

        logging.info("ğŸ‰ === æœ€ç»ˆè‰¾å®¾æµ©æ–¯è®­ç»ƒæŠ¥å‘Š ===")
        logging.info(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time:.0f}ç§’")
        logging.info(f"ğŸ”„ æ€»è®­ç»ƒæ­¥æ•°: {self.step_count}")
        logging.info(f"ğŸ“… æ€»è®­ç»ƒå‘¨æœŸ: {self.epoch_count}")
        logging.info(f"âœ… è’¸é¦åº”ç”¨æ¬¡æ•°: {self.distill_applied}")
        logging.info(f"ğŸ¯ æœ€ç»ˆå­¦ä¹ è¿›åº¦: {memory_report['learning_progress']:.1%}")
        logging.info(f"ğŸ“š æ€»è·Ÿè¸ªæ ·æœ¬: {memory_report['total_samples']}ä¸ª")
        logging.info(f"ğŸ’¾ æœ€ç»ˆè®°å¿†å¼ºåº¦: {memory_report['avg_intensity']:.3f}")
        logging.info(f"ğŸ“… æ€»å¤ä¹ å®‰æ’: {scheduling_report['total_reviews']}æ¬¡")

        if memory_report['total_samples'] > 0 and memory_report['learning_progress'] > 0:
            logging.info("âœ… è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒæˆåŠŸå®Œæˆ!")
        elif memory_report['total_samples'] > 0:
            logging.info("ğŸ”„ è‰¾å®¾æµ©æ–¯ç³»ç»Ÿå·²è·Ÿè¸ªæ ·æœ¬ï¼Œä½†å­¦ä¹ è¿›åº¦è¾ƒä½")
        else:
            logging.info("âš ï¸  è‰¾å®¾æµ©æ–¯ç³»ç»Ÿéœ€è¦æ”¹è¿›")

        logging.info("=" * 50)

    def train(self, epochs=100, imgsz=640, batch=16,  ** kwargs):
        """è®­ç»ƒå…¥å£"""
        logging.info("ğŸš€ å¼€å§‹è‰¾å®¾æµ©æ–¯åŠ¨æ€è’¸é¦è®­ç»ƒ...")

        train_config = {
            'data': self.data_config,
            'epochs': epochs,
            'imgsz': imgsz,
            'batch': batch,
            'device': self.device,
            'workers': kwargs.get('workers', 4),
            'lr0': kwargs.get('lr0', 0.01),
            'amp': kwargs.get('amp', True),
            'verbose': True,
            'project': kwargs.get('project', 'runs/ebbinghaus_distill'),
            'name': kwargs.get('name', f'ebbinghaus_{int(time.time())}'),
        }

        try:
            results = self.student.train(**train_config)
            logging.info("âœ… è‰¾å®¾æµ©æ–¯åŠ¨æ€è’¸é¦è®­ç»ƒå®Œæˆ")
            return results

        except Exception as e:
            logging.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise


class EbbinghausMemoryModel:
    """è‰¾å®¾æµ©æ–¯è®°å¿†æ¨¡å‹ - å®ç°æ‚¨æä¾›çš„ç†è®ºæ¡†æ¶"""

    def __init__(self):
        self.sample_memory_db = {}  # æ ·æœ¬è®°å¿†æ•°æ®åº“
        self.sample_difficulty_db = {}  # æ ·æœ¬éš¾åº¦æ•°æ®åº“
        self.global_memory_intensity = 0.5

        # è‰¾å®¾æµ©æ–¯å‚æ•°
        self.base_forgetting_rate = 0.95
        self.review_enhance_factor = 1.5
        self.learning_threshold = 0.7
        self.forgetting_threshold = 0.3

        # ç»Ÿè®¡
        self.total_samples = 0
        self.learned_samples = 0
        self.forgetting_samples = 0

        logging.info("ğŸ§  è‰¾å®¾æµ©æ–¯è®°å¿†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def compute_sample_difficulty(self, batch, index):
        """è®¡ç®—æ ·æœ¬éš¾åº¦ç³»æ•° - å®ç°æ‚¨æä¾›çš„å…¬å¼1"""
        sample_id = self._get_sample_id(batch, index)

        if sample_id in self.sample_difficulty_db:
            return self.sample_difficulty_db[sample_id]

        try:
            # ç®€åŒ–å®ç°ï¼šä½¿ç”¨å›¾åƒç»Ÿè®¡ä¿¡æ¯ä¼°è®¡éš¾åº¦
            if hasattr(batch, 'get'):
                imgs = batch.get('img')
                if imgs is not None and hasattr(imgs, 'shape') and index < imgs.shape[0]:
                    try:
                        if hasattr(imgs, '__getitem__'):
                            img_tensor = imgs[index]
                        else:
                            img_tensor = imgs

                        # å°†Tensorè½¬æ¢ä¸ºnumpyè¿›è¡Œè®¡ç®—
                        if img_tensor.dim() == 3:
                            img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)
                        else:
                            img_np = img_tensor.cpu().numpy()

                        # è®¡ç®—å„ç»´åº¦éš¾åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
                        width_difficulty = self._compute_width_difficulty(img_np)
                        contrast_difficulty = self._compute_contrast_difficulty(img_np)
                        complexity_difficulty = self._compute_complexity_difficulty(img_np)
                        background_difficulty = self._compute_background_difficulty(img_np)

                        # ç»¼åˆéš¾åº¦ç³»æ•° - å…¬å¼1
                        difficulty = (width_difficulty + contrast_difficulty +
                                      complexity_difficulty + background_difficulty) / 4.0

                        self.sample_difficulty_db[sample_id] = difficulty
                        return difficulty
                    except:
                        pass
        except:
            pass

        # é»˜è®¤ä¸­ç­‰éš¾åº¦
        return 1.0

    def _compute_width_difficulty(self, image_np):
        """è®¡ç®—å®½åº¦éš¾åº¦ D_width - ç®€åŒ–å®ç°"""
        try:
            if image_np.ndim == 3:
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            else:
                gray = image_np

            # ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹ä¼°è®¡è£‚ç¼
            edges = cv2.Canny(gray.astype(np.uint8), 50, 150)
            edge_density = np.sum(edges > 0) / edges.size

            # è¾¹ç¼˜å¯†åº¦ä½å¯èƒ½è¡¨ç¤ºç»†è£‚ç¼ï¼ˆéš¾åº¦é«˜ï¼‰
            width_difficulty = 1.0 + (1.0 - edge_density) * 2.0
            return min(3.0, max(0.5, width_difficulty))
        except:
            return 1.0

    def _compute_contrast_difficulty(self, image_np):
        """è®¡ç®—å¯¹æ¯”åº¦éš¾åº¦ D_contrast - ç®€åŒ–å®ç°"""
        try:
            if image_np.ndim == 3:
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            else:
                gray = image_np

            # ä½¿ç”¨æ ‡å‡†å·®è¡¡é‡å¯¹æ¯”åº¦
            contrast = np.std(gray)
            # å¯¹æ¯”åº¦ä½éš¾åº¦é«˜
            contrast_difficulty = 1.0 + (100 - min(contrast, 100)) / 100
            return min(3.0, max(0.5, contrast_difficulty))
        except:
            return 1.0

    def _compute_complexity_difficulty(self, image_np):
        """è®¡ç®—å½¢æ€å¤æ‚åº¦ D_complexity - ç®€åŒ–å®ç°"""
        try:
            if image_np.ndim == 3:
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            else:
                gray = image_np

            # ä½¿ç”¨å›¾åƒç†µä¼°è®¡å¤æ‚åº¦
            hist = cv2.calcHist([gray.astype(np.uint8)], [0], None, [256], [0, 256])
            hist = hist / hist.sum()
            entropy = -np.sum(hist * np.log2(hist + 1e-8))

            complexity_difficulty = 1.0 + entropy / 8.0  # å½’ä¸€åŒ–
            return min(3.0, max(0.5, complexity_difficulty))
        except:
            return 1.0

    def _compute_background_difficulty(self, image_np):
        """è®¡ç®—èƒŒæ™¯å¹²æ‰°åº¦ D_background - ç®€åŒ–å®ç°"""
        try:
            if image_np.ndim == 3:
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            else:
                gray = image_np

            # ä½¿ç”¨çº¹ç†å¤æ‚åº¦ä¼°è®¡èƒŒæ™¯å¹²æ‰°
            laplacian_var = cv2.Laplacian(gray.astype(np.uint8), cv2.CV_64F).var()
            background_difficulty = 1.0 + min(laplacian_var / 1000, 2.0)
            return min(3.0, max(0.5, background_difficulty))
        except:
            return 1.0

    def update_sample_memory(self, sample_id, learning_gain, current_step, difficulty, is_review=False):
        """æ›´æ–°æ ·æœ¬è®°å¿†å¼ºåº¦ - å®ç°æ‚¨æä¾›çš„å…¬å¼2,3"""
        if sample_id not in self.sample_memory_db:
            self.sample_memory_db[sample_id] = {
                'strength': 0.5,
                'last_learning_time': current_step,
                'learning_count': 0,
                'review_count': 0,
                'last_strength': 0.5
            }
            self.total_samples += 1

        record = self.sample_memory_db[sample_id]
        last_time = record['last_learning_time']
        time_gap = max(1, current_step - last_time)

        # è®°å¿†è¡°å‡ - å…¬å¼2
        base_decay = self.base_forgetting_rate
        difficulty_decay = base_decay * (1.0 + difficulty * 0.5)
        time_decay = math.exp(-time_gap / difficulty_decay)
        record['strength'] = record['strength'] * time_decay

        # è®°å¿†å¼ºåŒ– - å…¬å¼3
        if is_review:
            review_enhance = self.review_enhance_factor
        else:
            review_enhance = 1.0

        # åº”ç”¨å­¦ä¹ å¢ç›Š
        memory_increment = learning_gain * review_enhance
        record['strength'] = min(1.0, record['strength'] + memory_increment)

        # æ›´æ–°è®°å½•
        record['last_learning_time'] = current_step
        record['learning_count'] += 1
        if is_review:
            record['review_count'] += 1

        # æ£€æŸ¥å­¦ä¹ çŠ¶æ€
        old_strength = record.get('last_strength', 0.5)
        new_strength = record['strength']

        if old_strength < self.learning_threshold and new_strength >= self.learning_threshold:
            self.learned_samples += 1
        elif old_strength > self.forgetting_threshold and new_strength <= self.forgetting_threshold:
            self.forgetting_samples += 1

        record['last_strength'] = new_strength

        # æ›´æ–°å…¨å±€è®°å¿†å¼ºåº¦
        self._update_global_memory()

        return record['strength']

    def _update_global_memory(self):
        """æ›´æ–°å…¨å±€è®°å¿†å¼ºåº¦"""
        if not self.sample_memory_db:
            self.global_memory_intensity = 0.5
            return

        total_intensity = sum(record['strength'] for record in self.sample_memory_db.values())
        self.global_memory_intensity = total_intensity / len(self.sample_memory_db)

    def _get_sample_id(self, batch, index):
        """è·å–æ ·æœ¬ID"""
        try:
            if not hasattr(batch, 'get'):
                return f"batch_{index}"

            # ä½¿ç”¨å›¾åƒæ•°æ®ç”ŸæˆID
            imgs = batch.get('img')
            if imgs is not None and hasattr(imgs, 'shape') and index < imgs.shape[0]:
                try:
                    if hasattr(imgs, '__getitem__'):
                        img_tensor = imgs[index]
                    else:
                        img_tensor = imgs

                    if hasattr(img_tensor, 'mean') and hasattr(img_tensor, 'std'):
                        img_mean = img_tensor.mean().item()
                        img_std = img_tensor.std().item()

                        norm_mean = round(img_mean, 6)
                        norm_std = round(img_std, 6)

                        id_string = f"{norm_mean:.6f}_{norm_std:.6f}"
                        img_hash = hashlib.md5(id_string.encode()).hexdigest()[:8]
                        return f"img_{img_hash}"
                except:
                    pass

            return f"sample_{index}"
        except:
            return f"error_{index}"

    def get_memory_report(self):
        """è·å–è®°å¿†æŠ¥å‘Š"""
        if self.total_samples == 0:
            return {
                'total_samples': 0,
                'learned_samples': 0,
                'forgetting_samples': 0,
                'learning_progress': 0.0,
                'avg_intensity': 0.5,
                'health_score': 0.5
            }

        learning_progress = self.learned_samples / self.total_samples if self.total_samples > 0 else 0.0
        avg_intensity = self.global_memory_intensity

        # è®¡ç®—å¥åº·åº¦è¯„åˆ†
        forgetting_ratio = self.forgetting_samples / self.total_samples if self.total_samples > 0 else 0.0
        health_score = max(0.0, min(1.0, avg_intensity * 0.7 + (1 - forgetting_ratio) * 0.3))

        return {
            'total_samples': self.total_samples,
            'learned_samples': self.learned_samples,
            'forgetting_samples': self.forgetting_samples,
            'learning_progress': learning_progress,
            'avg_intensity': avg_intensity,
            'health_score': health_score
        }


class AdaptiveReviewScheduler:
    """è‡ªé€‚åº”å¤ä¹ è°ƒåº¦å™¨ - å®ç°æ‚¨æä¾›çš„ç®—æ³•1"""

    def __init__(self, memory_model):
        self.memory_model = memory_model
        self.review_count = 0
        self.last_review_step = 0
        self.adaptive_interval_adjustment = 1.0

        logging.info("ğŸ“… è‡ªé€‚åº”å¤ä¹ è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")

    def schedule_reviews(self, current_step, review_ratio=0.3):
        """è‡ªé€‚åº”å¤ä¹ è°ƒåº¦ç®—æ³• - ç®—æ³•1å®ç°"""
        if not self.memory_model.sample_memory_db:
            return []

        priorities = []

        for sample_id, memory_info in self.memory_model.sample_memory_db.items():
            forgetfulness = 1 - memory_info['strength']
            time_gap = current_step - memory_info['last_learning_time']
            difficulty = self.memory_model.sample_difficulty_db.get(sample_id, 1.0)
            learning_count = memory_info['learning_count']

            # å¤ä¹ ä¼˜å…ˆçº§è®¡ç®— - å…¬å¼4
            priority = forgetfulness * difficulty * math.log(1 + time_gap) / (1 + math.log(1 + learning_count))
            priorities.append((sample_id, priority, forgetfulness, difficulty, time_gap))

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priorities.sort(key=lambda x: x[1], reverse=True)

        # é€‰æ‹©å‰review_ratioçš„æ ·æœ¬
        num_review = int(len(priorities) * review_ratio)
        review_samples = [{
            'sample_id': sample_id,
            'priority': priority,
            'forgetfulness': forgetfulness,
            'difficulty': difficulty,
            'time_gap': time_gap
        } for sample_id, priority, forgetfulness, difficulty, time_gap in priorities[:num_review]]

        self.review_count += len(review_samples)
        self.last_review_step = current_step

        return review_samples

    def get_scheduling_report(self):
        """è·å–è°ƒåº¦æŠ¥å‘Š"""
        return {
            'total_reviews': self.review_count,
            'scheduled_reviews': self.review_count,
            'last_review_step': self.last_review_step,
            'adaptive_adjustment': self.adaptive_interval_adjustment
        }


class MemoryAwareDistillationLoss:
    """è®°å¿†æ„ŸçŸ¥è’¸é¦æŸå¤± - å®ç°æ‚¨æä¾›çš„å…¬å¼5-10"""

    def __init__(self, memory_model):
        self.memory_model = memory_model

        # è’¸é¦å‚æ•°
        self.base_temperature = 4.0
        self.temperature_adjust = 0.5
        self.memory_weight_factor = 0.7
        self.forgetting_tolerance = 0.1
        self.penalty_weight = 0.1

        logging.info("ğŸ¯ è®°å¿†æ„ŸçŸ¥è’¸é¦æŸå¤±åˆå§‹åŒ–å®Œæˆ")

    def compute(self, student_outputs, teacher_outputs, batch, current_step):
        """è®¡ç®—è®°å¿†æ„ŸçŸ¥è’¸é¦æŸå¤± - å…¬å¼5-10å®ç°"""
        try:
            # ç®€åŒ–å®ç°ï¼šè®¡ç®—åŸºç¡€è’¸é¦æŸå¤±
            base_loss = self._compute_base_distillation_loss(student_outputs, teacher_outputs)

            # åº”ç”¨è®°å¿†æ„ŸçŸ¥æƒé‡
            memory_weight = self._compute_memory_aware_weight(batch, current_step)
            weighted_loss = base_loss * memory_weight

            return weighted_loss

        except Exception as e:
            logging.warning(f"âŒ è®°å¿†æ„ŸçŸ¥è’¸é¦æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            return torch.tensor(0.05, requires_grad=True)

    def _compute_base_distillation_loss(self, student_out, teacher_out):
        """è®¡ç®—åŸºç¡€è’¸é¦æŸå¤±"""
        try:
            if isinstance(student_out, (list, tuple)) and isinstance(teacher_out, (list, tuple)):
                if len(student_out) > 0 and len(teacher_out) > 0:
                    s_feat = student_out[0]
                    t_feat = teacher_out[0]

                    # ç¡®ä¿å½¢çŠ¶åŒ¹é…
                    if s_feat.shape != t_feat.shape:
                        s_feat = F.interpolate(s_feat, size=t_feat.shape[2:], mode='bilinear')

                    # è®¡ç®—MSEæŸå¤±
                    return F.mse_loss(s_feat, t_feat) * 0.1
            return torch.tensor(0.05, requires_grad=True)
        except:
            return torch.tensor(0.05, requires_grad=True)

    def _compute_memory_aware_weight(self, batch, current_step):
        """è®¡ç®—è®°å¿†æ„ŸçŸ¥æƒé‡ - å…¬å¼6å®ç°"""
        try:
            if not hasattr(batch, 'get'):
                return 1.0

            # ç®€åŒ–å®ç°ï¼šä½¿ç”¨å…¨å±€è®°å¿†å¼ºåº¦
            memory_report = self.memory_model.get_memory_report()
            global_intensity = memory_report['avg_intensity']

            # è®°å¿†å¼ºåº¦ä½ -> é«˜æƒé‡
            memory_weight = (1 - global_intensity) * self.memory_weight_factor

            return max(0.5, min(2.0, 1.0 + memory_weight))
        except:
            return 1.0


def main():
    """ä¸»å‡½æ•°"""
    torch.manual_seed(42)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    print("=== è‰¾å®¾æµ©æ–¯åŠ¨æ€è’¸é¦è®­ç»ƒç³»ç»Ÿ ===")
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒ...")

    try:
        trainer = EbbinghausDynamicDistillation(
            teacher_path="runs/segment/train2/weights/best.pt",
            student_config="yolo11n-seg.yaml",
            data_config="./datasets/crack-seg/data.yaml",
            device=device
        )

        results = trainer.train(
            epochs=100,
            imgsz=640,
            batch=8,
            workers=0,
            lr0=0.001,
            amp=False
        )

        logging.info("ğŸ‰ è‰¾å®¾æµ©æ–¯åŠ¨æ€è’¸é¦è®­ç»ƒå®Œæˆ!")
        return results

    except Exception as e:
        logging.error(f"âŒ è‰¾å®¾æµ©æ–¯è’¸é¦è®­ç»ƒå¤±è´¥: {e}")
        return None


if __name__ == '__main__':
    main()