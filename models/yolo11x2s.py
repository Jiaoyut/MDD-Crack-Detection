# import os
# from ultralytics import YOLO
# import torch
# import torch.nn.functional as F
# import logging
#
# # è®¾ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
#
#
# class SimpleDistillationTrainer:
#     """ç®€åŒ–ç‰ˆè’¸é¦è®­ç»ƒå™¨ - æ›´å¯é """
#
#     def __init__(self, teacher_path, student_config, data_config, device='cuda'):
#         self.device = device
#
#         # åŠ è½½æ¨¡å‹
#         self.teacher = YOLO(teacher_path)
#         self.teacher.model.to(device).eval()
#         for p in self.teacher.model.parameters():
#             p.requires_grad = False
#
#         self.student = YOLO(student_config)
#         self.student.model.to(device)
#
#         self.data_config = data_config
#         self.step_count = 0
#
#         logging.info("âœ… ç®€åŒ–è’¸é¦è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
#
#     # def setup_distillation_callback(self):
#     #     """è®¾ç½®è’¸é¦å›è°ƒ"""
#     #
#     #     def on_train_batch_end(trainer):
#     #         self.step_count += 1
#     #
#     #         try:
#     #             # åŸºæœ¬æ£€æŸ¥
#     #             if not hasattr(trainer, 'loss') or trainer.loss is None:
#     #                 return
#     #             if not hasattr(trainer, 'batch') or trainer.batch is None:
#     #                 return
#     #
#     #             # è®°å½•åŸå§‹æŸå¤±
#     #             original_loss = trainer.loss.item()
#     #
#     #             # åº”ç”¨è’¸é¦æŸå¤±
#     #             distill_loss = torch.tensor(0.05, device=self.device, requires_grad=True)
#     #             trainer.loss = trainer.loss + 0.3 * distill_loss
#     #
#     #             # è®°å½•æ—¥å¿—
#     #             if self.step_count % 100 == 0:
#     #                 new_loss = trainer.loss.item()
#     #                 logging.info(f"ğŸ¯ æ­¥éª¤ {self.step_count}: è’¸é¦åº”ç”¨æˆåŠŸ")
#     #                 logging.info(f"ğŸ’§ æŸå¤±å˜åŒ–: {original_loss:.4f} -> {new_loss:.4f}")
#     #
#     #         except Exception as e:
#     #             if self.step_count % 200 == 0:
#     #                 logging.warning(f"è’¸é¦å›è°ƒé”™è¯¯: {e}")
#     #
#     #     # æ³¨å†Œå›è°ƒ
#     #     if not hasattr(self.student, 'callbacks'):
#     #         self.student.callbacks = {}
#     #     if 'on_train_batch_end' not in self.student.callbacks:
#     #         self.student.callbacks['on_train_batch_end'] = []
#     #     self.student.callbacks['on_train_batch_end'].append(on_train_batch_end)
#
#     def setup_distillation_callback(self):
#         """è®¾ç½®è’¸é¦å›è°ƒ"""
#
#         # è‰¾å®¾æµ©æ–¯è’¸é¦å‚æ•°
#         w0 = 0.3  # åˆå§‹æƒé‡
#         tau = 2000  # è¡°å‡é€Ÿåº¦ï¼ˆå¯è°ƒï¼‰
#
#         def on_train_batch_end(trainer):
#             self.step_count += 1
#
#             try:
#                 if not hasattr(trainer, 'loss') or trainer.loss is None:
#                     return
#                 if not hasattr(trainer, 'batch') or trainer.batch is None:
#                     return
#
#                 # åŸå§‹æŸå¤±
#                 original_loss = trainer.loss.item()
#
#                 # ---------------------------
#                 # ğŸ”¥ è®¡ç®—è‰¾å®¾æµ©æ–¯è’¸é¦æƒé‡
#                 # ---------------------------
#                 ebbinghaus_weight = w0 * torch.exp(
#                     torch.tensor(- self.step_count / tau, device=self.device)
#                 )
#
#                 # ---------------------------
#                 # ğŸ”¥ è®¡ç®— teacher-student logits è’¸é¦æŸå¤±
#                 # ---------------------------
#                 # teacher å‰å‘
#                 with torch.no_grad():
#                     t_out = self.teacher.model(trainer.batch['img'].to(self.device))
#
#                 # student å‰å‘ï¼ˆYOLO æ¡†æ¶ä¸­ trainer.loss å·²ç»ç®—è¿‡ï¼Œè¿™é‡Œé‡æ–° forward æ˜¯å¿…é¡»çš„ï¼‰
#                 s_out = self.student.model(trainer.batch['img'].to(self.device))
#
#                 # ç®€åŒ–ï¼šåªå¯¹æœ€åä¸€å±‚è¾“å‡ºè¿›è¡Œè’¸é¦ï¼ˆæœ€å¸¸ç”¨ï¼‰
#                 # ä½ ä¹Ÿå¯ä»¥åš KL/Feature distillation
#                 distill_loss = F.mse_loss(s_out[0], t_out[0])
#
#                 # ---------------------------
#                 # ğŸ”¥ ç»„åˆæ€»æŸå¤±
#                 # ---------------------------
#                 trainer.loss = trainer.loss + ebbinghaus_weight * distill_loss
#
#                 # æ‰“å°æ—¥å¿—
#                 if self.step_count % 100 == 0:
#                     logging.info(f"ğŸ¯ Step {self.step_count}: Ebbinghaus Weight = {ebbinghaus_weight.item():.6f}")
#                     logging.info(f"ğŸ’§ Distill Loss = {distill_loss.item():.4f}")
#                     logging.info(f"ğŸ“‰ Total Loss: {original_loss:.4f} -> {trainer.loss.item():.4f}")
#
#             except Exception as e:
#                 if self.step_count % 200 == 0:
#                     logging.warning(f"è’¸é¦å›è°ƒé”™è¯¯: {e}")
#
#         # æ³¨å†Œå›è°ƒ
#         if not hasattr(self.student, 'callbacks'):
#             self.student.callbacks = {}
#         if 'on_train_batch_end' not in self.student.callbacks:
#             self.student.callbacks['on_train_batch_end'] = []
#         self.student.callbacks['on_train_batch_end'].append(on_train_batch_end)
#
#     def train(self,  ** kwargs):
#         """è®­ç»ƒæ–¹æ³•"""
#         logging.info("ğŸš€ å¼€å§‹ç®€åŒ–è’¸é¦è®­ç»ƒ...")
#
#         # è®¾ç½®è’¸é¦å›è°ƒ
#         self.setup_distillation_callback()
#
#         # è®­ç»ƒé…ç½®
#         config = {
#             'data': self.data_config,
#             'epochs': kwargs.get('epochs', 100),
#             'imgsz': kwargs.get('imgsz', 640),
#             'batch': kwargs.get('batch', 8),
#             'device': self.device,
#             'workers': kwargs.get('workers', 0),
#             'lr0': kwargs.get('lr0', 0.001),
#             'amp': kwargs.get('amp', False),
#             'verbose': True,
#         }
#
#         results = self.student.train(**config)
#         logging.info(f"âœ… ç®€åŒ–è’¸é¦è®­ç»ƒå®Œæˆï¼Œæ€»æ­¥éª¤: {self.step_count}")
#         return results
#
#
# def main():
#     """ä¸»å‡½æ•°"""
#     try:
#         trainer = SimpleDistillationTrainer(
#             teacher_path="runs/segment/train2/weights/best.pt",
#             student_config="yolo11n-seg.yaml",
#             data_config="./datasets/crack-seg/data.yaml",
#             device='cuda' if torch.cuda.is_available() else 'cpu'
#         )
#
#         results = trainer.train(
#             epochs=100,
#             imgsz=640,
#             batch=8,
#             workers=0,
#             lr0=0.001,
#             amp=False
#         )
#
#         logging.info("ğŸ‰ è’¸é¦è®­ç»ƒå®Œæˆ!")
#         return results
#
#     except Exception as e:
#         logging.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
#         return None
#
#
# if __name__ == '__main__':
#     main()





import os
from ultralytics import YOLO
import torch
# torch.backends.cudnn.enabled = False
import torch.nn.functional as F
import logging
import math
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'


class SimpleDistillationTrainer:
    """ç®€åŒ–ç‰ˆè’¸é¦è®­ç»ƒå™¨ - å¢åŠ è®°å¿†å»ºæ¨¡ + åŠ¨æ€å¤ä¹ è°ƒåº¦ + è®°å¿†æ„ŸçŸ¥è’¸é¦"""

    def __init__(self, teacher_path, student_config, data_config, device='cuda'):
        self.device = device

        # åŠ è½½æ¨¡å‹
        self.teacher = YOLO(teacher_path)
        self.teacher.model.to(device).eval()
        for p in self.teacher.model.parameters():
            p.requires_grad = False

        self.student = YOLO(student_config)
        self.student.model.to(device)

        self.data_config = data_config
        self.step_count = 0

        # ---------- Memory modules state ----------
        # memory_store: sample_id -> dict(strength: float [0,1], last_review: int, next_review: int)
        self.memory_store = defaultdict(lambda: {'strength': 0.0, 'last_review': -1, 'next_review': 0})
        # per-sample history (optional): could store last loss etc
        self.history = defaultdict(list)

        # ---------- Hyperparams for Ebbinghaus+memory ----------
        self.ebbinghaus_w0 = 0.3   # initial base distill weight
        self.ebbinghaus_tau = 2000  # global exponential decay tau

        # memory update params
        self.memory_alpha = 0.3    # æ›´æ–°è®°å¿†å¼ºåº¦çš„é€Ÿç‡ (EMA)
        self.max_strength = 0.999
        self.min_strength = 0.0

        # dynamic review scheduling params
        self.base_interval = 500    # åŸºæœ¬å¤ä¹ é—´éš”ï¼ˆæ­¥éª¤ï¼‰
        self.interval_scale = 4.0   # strength è¶Šå¤§ï¼Œé—´éš”è¶Šé•¿ (next_interval = base_interval * (1 + strength*interval_scale))
        self.minimum_interval = 50  # æœ€å°é—´éš”

        # threshold: å½“ step >= next_review æ—¶ï¼Œä¼šå¯¹è¯¥æ ·æœ¬è¿›è¡Œâ€œå¤ä¹ /è’¸é¦â€
        # memory-aware weighting: memory_factor = (1 - strength) ** power
        self.memory_power = 1.0

        logging.info("âœ… ç®€åŒ–è’¸é¦è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    # ----------------- è¾…åŠ©å‡½æ•° -----------------
    def extract_sample_ids(self, batch):
        """
        ä» trainer.batch å°è¯•æå–æ ·æœ¬å”¯ä¸€ id åˆ—è¡¨ï¼ˆé•¿åº¦ == batch_sizeï¼‰
        æ”¯æŒå¤šä¸ªå¯èƒ½å‡ºç°çš„å­—æ®µåï¼›è‹¥å‡æœªæ‰¾åˆ°ï¼Œåˆ™é€€å›ç”Ÿæˆä¼ª idï¼ˆstep_indexï¼‰
        è¿”å›: list of sample_id (str)
        """
        # å¸¸è§ ultralytics batch å­—æ®µå°è¯•ï¼ˆæ ¹æ®ä½ æ¡†æ¶çš„å®é™…å­—æ®µåšè°ƒæ•´ï¼‰
        if isinstance(batch, dict):
            for k in ('img_files', 'file_name', 'filenames', 'files', 'image_path', 'paths'):
                if k in batch and batch[k] is not None:
                    # æœŸå¾…æ˜¯ list-like
                    try:
                        return [str(x) for x in batch[k]]
                    except Exception:
                        pass

            # æœ‰çš„è®­ç»ƒå™¨æŠŠ images æ”¾åœ¨ batch['imgs'] / batch['img']
            # æˆ‘ä»¬æ— æ³•ä» tensor æœ¬èº«è·å¾—æ–‡ä»¶åï¼›å›é€€åˆ° index-based ids
        # fallback: create stable ids based on current global step and local batch index
        batch_size = None
        if isinstance(batch, dict) and 'img' in batch:
            try:
                batch_size = batch['img'].shape[0]
            except Exception:
                batch_size = None
        # å¦‚æœè¿˜èƒ½æ‹¿åˆ° trainer.batch[0] style, ä¹Ÿå¯æ£€æŸ¥
        if batch_size is None:
            # æœ€åæ‰‹æ®µï¼šè®¾ä¸º 1 å¹¶è¿”å› a single pseudo id
            return [f"pseudo_{self.step_count}_0"]
        else:
            return [f"pseudo_{self.step_count}_{i}" for i in range(batch_size)]

    def compute_next_review_interval(self, strength):
        """
        åŸºäºå½“å‰è®°å¿†å¼ºåº¦è®¡ç®—ä¸‹ä¸€æ¬¡å¤ä¹ çš„é—´éš”ï¼ˆstepsï¼‰ã€‚
        strength âˆˆ [0,1]ï¼Œstrength è¶Šé«˜ï¼Œé—´éš”è¶Šé•¿ã€‚
        """
        interval = int(self.base_interval * (1.0 + strength * self.interval_scale))
        return max(self.minimum_interval, interval)

    def update_memory_strength(self, sample_id, review_loss_norm):
        """
        ä½¿ç”¨ç®€å• EMA æ›´æ–°è®°å¿†å¼ºåº¦ï¼š
          æ–°_strength = (1 - alpha) * old + alpha * (1 - normalized_loss)
        normalized_loss æœŸæœ› âˆˆ [0,1], è¶Šå°è¡¨ç¤ºæŒæ¡è¶Šå¥½ã€‚
        """
        old = self.memory_store[sample_id]['strength']
        new = (1.0 - self.memory_alpha) * old + self.memory_alpha * (1.0 - review_loss_norm)
        new = max(self.min_strength, min(self.max_strength, new))
        self.memory_store[sample_id]['strength'] = new
        return new

    def normalize_loss_for_memory(self, loss_value):
        """
        å°† distill_loss æˆ– review æå¤± æ ‡å‡†åŒ–åˆ° [0,1] ç”¨æ¥æ›´æ–°è®°å¿†å¼ºåº¦ã€‚
        è¿™é‡Œé‡‡ç”¨ç®€å•çš„å¹³æ»‘å½’ä¸€ï¼ˆå¯ä»¥æ¢æˆæ›´å¤æ‚çš„ percentile æˆ– running-statï¼‰
        """
        # simple soft normalization: sigmoid-like mapping scaled by a factor
        # å…ˆ clamp loss to a reasonable range
        val = float(loss_value)
        val = max(0.0, val)
        # å‚æ•° c æ§åˆ¶å°ºåº¦ï¼Œè¶Šå¤§è¶Šå¹³ç¼“ã€‚å¯è°ƒã€‚
        c = 1.0
        norm = 1.0 - (1.0 / (1.0 + val / c))  # maps 0->0, large->1
        # invert so that small loss -> small norm ; we want normalized_loss in [0,1] with small meaning mastered
        return min(1.0, max(0.0, norm))

    # ----------------- ä¸»å›è°ƒï¼ˆå¸¦è®°å¿†çš„è‰¾å®¾æµ©æ–¯è’¸é¦ï¼‰ -----------------
    def setup_distillation_callback(self):
        """è®¾ç½®è’¸é¦å›è°ƒï¼ˆå«è®°å¿†å»ºæ¨¡ + åŠ¨æ€å¤ä¹ è°ƒåº¦ + è®°å¿†æ„ŸçŸ¥æŸå¤±ï¼‰"""

        def on_train_batch_end(trainer):
            self.step_count += 1

            try:
                # åŸºæœ¬æ£€æŸ¥
                if not hasattr(trainer, 'loss') or trainer.loss is None:
                    return
                if not hasattr(trainer, 'batch') or trainer.batch is None:
                    return

                # è®°å½•åŸå§‹æŸå¤±
                original_loss = trainer.loss.item()

                # ------------- è®¡ç®— Ebbinghaus å…¨å±€æƒé‡ -------------
                ebbinghaus_weight = self.ebbinghaus_w0 * math.exp(- self.step_count / max(1.0, self.ebbinghaus_tau))
                ebbinghaus_weight = float(ebbinghaus_weight)  # convert to python float; will wrap as tensor later

                # ------------- æå–æ ·æœ¬ idsï¼ˆé•¿åº¦ == batch sizeï¼‰ -------------
                sample_ids = self.extract_sample_ids(trainer.batch)

                # ------------- teacher / student å‰å‘ï¼ˆæŒ‰ batchï¼‰ -------------
                # è¿™é‡Œå¯¹ batch_image åšä¸€æ¬¡ teacher forward (no_grad) å¹¶ student forwardï¼ˆç”¨äºè®¡ç®— distillation targetsï¼‰
                imgs = trainer.batch.get('img') or trainer.batch.get('imgs')  # å¯èƒ½é”®åä¸åŒ
                if imgs is None:
                    # æ— æ³•è¿›è¡ŒåŸºäºå›¾åƒçš„è’¸é¦ â€” ä»…è®°å½•å¹¶è¿”å›
                    if self.step_count % 200 == 0:
                        logging.warning("batch ä¸­æœªæ‰¾åˆ° 'img' æˆ– 'imgs'ï¼Œæ— æ³•æ‰§è¡Œå›¾åƒçº§è’¸é¦ã€‚")
                    return

                imgs = imgs.to(self.device)

                with torch.no_grad():
                    t_out = self.teacher.model(imgs)

                s_out = self.student.model(imgs)

                # ------------- è®¡ç®— per-sample distillation loss & memory-aware weighting -------------
                # æˆ‘ä»¬å‡è®¾ t_out å’Œ s_out çš„ç¬¬0é¡¹ä¸º feature/logit å¼ é‡ï¼Œä¸” batch å¯¹åº”é¡ºåºä¸€è‡´
                # ä½ å¯æ ¹æ®å…·ä½“ teacher/student è¾“å‡ºç»“æ„ä¿®æ”¹ç´¢å¼•
                # å¦‚æœè¾“å‡ºä¸æ˜¯ list/tupleï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å¼ é‡
                def get_primary_output(x):
                    if isinstance(x, (list, tuple)):
                        return x[0]
                    return x

                t_primary = get_primary_output(t_out)
                s_primary = get_primary_output(s_out)

                # è‹¥ primary è¾“å‡º shape ä¸ batch size ä¸ä¸€è‡´ï¼Œå›é€€å¹¶åšæ•´ä½“ mse
                try:
                    # distill_per_sample: list len == batch_size
                    bs = s_primary.shape[0]
                except Exception:
                    bs = None

                total_distill = torch.tensor(0.0, device=self.device)
                active_reviews = 0

                # iterate per sample and decide whether to "review"
                for i, sid in enumerate(sample_ids):
                    # sanity: ensure i within bs
                    if bs is None or i >= bs:
                        break

                    mem = self.memory_store[sid]
                    # å¦‚æœé¦–æ¬¡é‡åˆ°è¯¥æ ·æœ¬ï¼Œmem fields æœ‰é»˜è®¤å€¼
                    if mem['last_review'] < 0:
                        mem['strength'] = 0.0
                        mem['last_review'] = 0
                        mem['next_review'] = 0  # ç«‹å³å¤ä¹ ä¸€æ¬¡ï¼Œè®©ç¬¬ä¸€æ¬¡å……åˆ†å¸æ”¶

                    # check if this sample is scheduled for review now
                    if self.step_count >= mem['next_review']:
                        # è®¡ç®—å•æ ·æœ¬è’¸é¦æŸå¤±ï¼ˆMSE between student feature and teacher featureï¼‰
                        # å–æ¯æ ·æœ¬çš„ feature å‘é‡/å¼ é‡ slice
                        s_feat = s_primary[i:i+1]
                        t_feat = t_primary[i:i+1]

                        # è‹¥å½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯• squeezeå¹¶broadcast
                        try:
                            distill_i = F.mse_loss(s_feat, t_feat, reduction='mean')
                        except Exception:
                            # fallback: scalar 0
                            distill_i = torch.tensor(0.0, device=self.device)

                        # memory factor: é«˜ strength -> less weight (å› ä¸ºæŒæ¡å¾—å¥½ä¸éœ€è¿‡å¤šå¤ä¹ )
                        strength = float(mem['strength'])
                        memory_factor = (1.0 - strength) ** self.memory_power

                        # final per-sample weight = ebbinghaus_weight * memory_factor
                        weight_i = ebbinghaus_weight * memory_factor

                        # accumulate weighted distill loss
                        total_distill = total_distill + (weight_i * distill_i)
                        active_reviews += 1

                        # æ›´æ–°è®°å¿†ï¼šç”¨å½’ä¸€åŒ– loss æ›´æ–° strength
                        # æ³¨æ„ï¼šnormalize_loss_for_memory çš„è¾“å‡ºè¶Šå¤§è¡¨ç¤ºâ€œå›°éš¾/å¿˜è®°â€ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ 1 - norm è¡¨ç¤ºæŒæ¡åº¦
                        norm_loss = self.normalize_loss_for_memory(distill_i.item())
                        # norm_loss approx in [0,1] with 0 indicating very easy (good), large -> >0
                        new_strength = self.update_memory_strength(sid, norm_loss)
                        mem['last_review'] = self.step_count
                        mem['next_review'] = self.step_count + self.compute_next_review_interval(new_strength)

                        # è®°å½•å†å²ï¼ˆå¯é€‰ï¼‰
                        self.history[sid].append({
                            'step': self.step_count,
                            'distill_loss': float(distill_i.item()),
                            'weight': weight_i,
                            'strength': new_strength
                        })

                # è‹¥æœ‰è¢«å¤ä¹ çš„æ ·æœ¬ï¼Œåˆ™å°† distill åŠ å…¥æ€»æŸå¤±
                if active_reviews > 0:
                    # total_distill å·²ç»æ˜¯åŠ æƒå’Œï¼›å¯ä»¥é€‰æ‹©é™¤ä»¥ active_reviews åšå¹³å‡
                    avg_distill = total_distill / max(1, active_reviews)
                    trainer.loss = trainer.loss + avg_distill
                else:
                    # æ²¡æœ‰æ ·æœ¬éœ€è¦å¤ä¹  -> ä¸é¢å¤–åŠ æŸå¤±
                    avg_distill = None

                # æ‰“å°æ—¥å¿—ï¼ˆå¶å°”ï¼‰
                if self.step_count % 100 == 0:
                    if avg_distill is not None:
                        logging.info(f"ğŸ¯ æ­¥éª¤ {self.step_count}: Ebbinghaus base w={ebbinghaus_weight:.6f}, active_reviews={active_reviews}, avg_distill={avg_distill.item():.6f}")
                    else:
                        logging.info(f"ğŸ¯ æ­¥éª¤ {self.step_count}: æ— éœ€å¤ä¹ æ ·æœ¬ (active_reviews=0). Ebbinghaus w={ebbinghaus_weight:.6f}")
                    # è¾“å‡ºä¸€äº› sample çš„ strength å¿«ç…§
                    sample_snapshot = list(self.memory_store.items())[:3]
                    snap_str = ", ".join([f"{k}:{v['strength']:.3f}" for k, v in sample_snapshot])
                    logging.info(f"ğŸ§  memory snapshot (first 3): {snap_str}")

            except Exception as e:
                if self.step_count % 200 == 0:
                    logging.warning(f"è’¸é¦å›è°ƒé”™è¯¯: {e}")

        # æ³¨å†Œå›è°ƒ
        if not hasattr(self.student, 'callbacks'):
            self.student.callbacks = {}
        if 'on_train_batch_end' not in self.student.callbacks:
            self.student.callbacks['on_train_batch_end'] = []
        self.student.callbacks['on_train_batch_end'].append(on_train_batch_end)

    def train(self,  ** kwargs):
        """è®­ç»ƒæ–¹æ³•"""
        logging.info("ğŸš€ å¼€å§‹ç®€åŒ–è’¸é¦è®­ç»ƒ...")

        # è®¾ç½®è’¸é¦å›è°ƒ
        self.setup_distillation_callback()

        # è®­ç»ƒé…ç½®
        config = {
            'data': self.data_config,
            'epochs': kwargs.get('epochs', 100),
            'imgsz': kwargs.get('imgsz', 640),
            'batch': kwargs.get('batch', 8),
            'device': self.device,
            'workers': kwargs.get('workers', 0),
            'lr0': kwargs.get('lr0', 0.001),
            'amp': kwargs.get('amp', False),
            'verbose': True,
        }

        results = self.student.train(**config)
        logging.info(f"âœ… ç®€åŒ–è’¸é¦è®­ç»ƒå®Œæˆï¼Œæ€»æ­¥éª¤: {self.step_count}")
        return results


# ---------- main ä¿æŒä¸å˜ ----------
def main():
    """ä¸»å‡½æ•°"""
    try:
        trainer = SimpleDistillationTrainer(
            teacher_path="runs/segment/train2/weights/best.pt",
            student_config="yolo11n-seg.yaml",
            data_config="./datasets/crack-seg/data.yaml",
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )

        results = trainer.train(
            epochs=100,
            imgsz=640,
            batch=8,
            workers=0,
            lr0=0.001,
            amp=False
        )

        logging.info("ğŸ‰ è’¸é¦è®­ç»ƒå®Œæˆ!")
        return results

    except Exception as e:
        logging.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return None


if __name__ == '__main__':
    main()
